---
title: "Time Series and Forecasting"
author: "Ji Hun Lee"
date: "May 16, 2020"
output: html_document
---

# Introduction

The goal of this post is to show my workflow in forecasting time series data in R. Time series are almost ubiquitous and have the unique property in which data is sequential and have dependency structure.We are interested in making predictions about the future in many situations: deciding whether to build another power generation plant in the next ten years requires forecasts of future demand; scheduling staff in a call center next week requires forecasts of call volumes; stocking an inventory requires forecasts of stock requirements. Forecasts can be required several years in advance (for the case of capital investments), or only a few minutes beforehand (for telecommunication routing). Whatever the circumstances or time horizons involved, forecasting is an important aid to effective and efficient planning. 

# My Workflow

My workflow follows seven steps from examining data to making inferences based on the model fit. 

I. Examine the Structure of Data

  Understand a problem at hand, and determine whether my data is one of the three types.

  - Univariate or Multiple Independent time series 
  - Regression time series 
  - Hierarchical time series
  
II. Preprocess Datetime Data

  Conform a datetime column and extract useful features from it. I typically use lubridate. For importing regularly spaced time series, I use xts and transform into R's ts object.

  - Parsing and transforming time objects with lubridate
  - Reading time series with xts

III. EDA and Visualization

  Analyze my variable(s) of interest. The way I tackle visualization depends on the structure of data. I always proceed with the visualizations below on any data structure.
  
  - Location, Dispersion, and Distribution 
  - (Multiple) Line Chart: at every unit of date (year, month, day)
  - Correlogram (only for multiple time series)
  - Trend, Seasonality and Cycle of Time Series
  - Stationarity and Autocorrelogram
  
  For in-depth analysis and presentation purposes, I do the following visualizations.
  
  - Bar Chart: by day of week, by hour of day
  - Slope Chart: How does today compare with the start of a time period?
  - Cycle Chart: Are there cyclical patterns in data?
  - Calendar Heatmap: How can I look up trends across two time dimensions? 
  
  In dealing with regression and hierarchical structure of data, I bolster the visualizations above using interactivity of library(plotly).
  
IV. Feature Engineering based on EDA

  We have already done half of feature engineering work on datetime preprocessing. We also need to transform and engineer new variables based on insights from EDA.

  - variance stabilizing transform
  - smoothing
  - differencing
  - log differencing
  - seasonal differencing

V. Models and Forecasting

  Now that data are clean and ready for modeling, we use the following models. Data structures have their own different models. 

  Univariate and Multiple Independent Time Series:
  
  - Simple Models (Naive, MA)
  - Linear Regression
  - Splines
  - Exponential Smoothing
  - ARIMA
  - ARFIMA
  - SARIMA
  - GARCH
  - Complex seasonalities
  - VARIMA
  - TBAT
  - Neural network
  - RNN
  
  Regression Time Series:
  
  - Linear Regression
  - ARIMA
  
  Hierarchical Time Series:
  
  - Hierarchical Time Series
  
VI. Model Evaluation and Selection

  - Statistical models: AIC, hypothesis testing, graphical inspection
  - General models: time series cross validation
  
VII. Inference

  - Confidence Interval

####  Import Relevant Libraries

```{r message=FALSE}
library(xts) # contains zoo, extensible time series, conversion to ts object
library(TSA) # DirRec(), beersales data
library(hms) # hms object
library(fpp2) # conntains various data
library(urca) # unit root test 
library(vars) # vector autoregressions
library(astsa) # applied statistical time series analysis; ARIMA models
library(readr) # read csv file more efficiently
library(fGarch) # garchFit()
library(readxl) # read excel file
library(tseries) # ADF and KPSS tests
library(anytime) # POSIXct and Date converter
library(ggplot2) # visualiztion, autoplot
library(forecast) # forecast(), auto.arima(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf, ggPcf datasets: gold, woolyrnq, gas
library(fracdiff) # fractional ARIMA
library(fasttime) # fasster parsing than lubridate, fastPOSIXct()
library(tidyverse) # data manipulation
library(lubridate) # date object parsing, extraction, rounding
library(DataCombine) # slide()
library(PerformanceAnalytics) # chart.TimeSeries()
```

# I. Data

It is very important at the first step to see what is the objective of the study is and what kinds of data are given. Depending on the types of data given, we apply different models. 

### 1. Univariate Time Series

Univariate time series is one in which only one variable is observed repeatedly over time. If the goal is to predict multiple variables independently, then it can be classified as univariate time series because we can apply our analysis on each time series.

1. Monthly Australian gas production 1956–1995 from Australian Bureau of Statistics.
2. Monthly Beer Sales in millions of barrels, 01/1975 - 12/1990.
3. Hourly traffic flow in Chicago
```{r}
#  monthly gas prices
data(gas)
# monthly gold production
data(gold)
#  . Beer Sales
data(beersales)
```

### 2. Multivariate Time Series

Multivariate time series is one in which exogenous variables are regressed with the main variable of interest. Once the variable of interest is chosen, we want to know its relationship to other variables. 

1. Daily stock exchange returns from global stock exchanges
2. Quarterly GDP and Unemployment Rate from 1955 to 1969 for United Kingdom
3. Monthly Chicago mercantile exchange data; it contains seat prices from Chicago Mercantile Exchange. There are three classes of seats CME, IMM, and IOM which confer the rights to trade ifferent sets of commodities traded. CME seat owners can trade everything; IMM seat owners are allowed to trade everything except agricultural products; and seat owners are allowed to trade only in index products and all options. For each class of seat, we need to forecast time series of seat price for each month from January 2001 to December 2013 using trading volume.
```{r}
#  . Istanbul Stock Exchange
stock_exchange <- read_csv('C:/Users/jihun/Downloads/time_series/istanbul_stock_exchange.csv')
#  . US GDP and Unemployment Rate
gdp <- read_excel('C:/Users/jihun/Downloads/time_series/UK_GDP_Unemployment.xlsx')
#  . Chicago traffic flow
# traffic  
#  . CO2 Emission
co2 <- read_csv('C:/Users/jihun/Downloads/time_series/CO2_emission.csv')
#  . Chicago Mercantile Exchange 
list <- read_list('C:/Users/jihun/Downloads/time_series/chicago_mercantile_exchange/')
Contracts_Volume %>%
  inner_join(Contracts_Classification) %>% 
  inner_join(cmeS) %>% 
  inner_join(immS) %>% 
  inner_join(iomS)
```

### 3. Hierarchicagl Time Series

Hierarchical time series is one in which variables can form hierarchical structure such as items and product categories that are themselves forming another higher category. We are interested in detecting aggregate effects as well as individual effects.

1. Hourly Energy Data from IECS
2. Daily Walmart Sales data from 
```{r}
#  . Energy Usage

#  . Walmart Sales

```


### Difference between Time Series and Longitudinal Data

Longitudinal data is one in which multiple similar units are gone through some treatment and observed over time. Usually there are many units and fewer observations across time. They receive a separate treatment.

One key difference seems to be that longitudinal data is often used in causal analyses, to understand the impact of interventions or treatments, whereas time series are often used in forecasting. Of course, the difference is not clear-cut (you need to understand the underlying drivers to forecast, and IMO you haven't understood the drivers unless you can forecast well). But people who do signal detection in time series often don't care so much about forecasting, so they would probably reject my distinction.

In general however, time series connotes a single study unit observed at regular intervals over a very long period of time. A prototypical example would be the annual GDP growth of a country over decades or even more than a hundred years. For an analyst working for a private company, it might be monthly sales revenues over the life of the company. Because there are so many observations, the data are analyzed in great detail, looking for things like seasonality over different periods (e.g., monthly: more sales at the beginning of a month just after people have been paid; yearly: more sales in November and December, when people are shopping for the Christmas season), and possibly regime shifts. Forecasting is often very important.

Longitudinal typically refers to fewer measurements over a larger number of study units. A prototypical example might be a drug trial, where there are hundreds of patients measured at baseline (before treatment), and monthly for the next 3 months. With just 4 observations of each unit in this example, it is not possible to try to detect the kinds of features time series analysts are interested in. On the other hand, with patients presumably randomized into treatment and control arms, causality can be inferred once the non-independence has been addressed. As that suggests, often the non-independence is considered almost a nuisance, rather than the primary feature of interest.

cross section: different subjects at the same time; think of it as one row with many columns corresponding to different subjects;
time series: the same subject at different times; think of it as one column with rows corresponding to different time points;
panel (longitudinal): many subjects at different times, you have the same subject at different times, and you have many subjects at the same time; think of it as a table where rows are time points, and columns are subjects.


# II. Preprocessing: Date Objects in R

The goal of preprocessing is to conform date data into R's date object, which has to follow ISO-8601 protocol. This section will also cover two major datetime libraries in R - lubridate and xts - and see how they can be used to extract useful information from datetime object. 

Do not skip this section if:

  - date column does not follow ISO-8601 format.
  - need to extract datetime components or combine individual datetime components
  - need to adjust time zone
  - need to make use of time intervals
  - need to make use of weekday/weekend/holiday information

Date is a special type of data in R. I will review some key functions in R base function and class, lubridate, and xts packagesthat help us maniupuate time data. These functions will be key in feature extraction of time related information.

## R's base Date object

We can read date as factor or character, but we should read it as date object because we can work with it more easily. R's dates act like numbers in that inequality relation holds and we can do arithmetic on it as well as other numeric operations such as max() and min().

When we want to visualize dates in ggplot2, we can can specify the limits by xlim or scale_x_Date functions

When we import raw data, it is usually read as a string, and we need R to recognize a string as a datetime by converting it with the functions as.POSIXct(). as.POSIXct() expects strings to be in the format YYYY-MM-DD HH:MM:SS. The function is not as flexible and expects the string to follow a strict stanard of ISO-8601.

What is POSIXct? Class "POSIXct" represents the (signed) number of seconds since the beginning of 1970 (in the UTC time zone) as a numeric vector. Class "POSIXlt" is a named list of vectors representing sec. 0–61: seconds.

What is ISO8601? Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization and was first published in 1988.

The base functions are:
  - as.POSIXct()
  - as.Date()
  - strptime() 
  - format()
  - ISOdate()
  - difftime()
  
### Parsing and Reformat Time Date

Code	Meaning	          Code	Meaning
%a	Abbreviated weekday	 %A	Full weekday
%b	Abbreviated month   	%B	Full month
%c	Locale-specific date and time	%d	Decimal date
%H	Decimal hours (24 hour)	%I	Decimal hours (12 hour)
%j	Decimal day of the year	%m	Decimal month
%M	Decimal minute	%p	Locale-specific AM/PM
%S	Decimal second	%U	Decimal week of the year (starting on Sunday)
%w	Decimal Weekday (0=Sunday)	%W	Decimal week of the year (starting on Monday)
%x	Locale-specific Date	%X	Locale-specific Time
%y	2-digit year	%Y	4-digit year
%z	Offset from GMT	%Z	Time zone (character)

We can parse data that do not follow ISO-8601 format.
```{r}
mydate = strptime('16/Oct/2005:07:51:00',format='%d/%b/%Y:%H:%M:%S')
format(thedate,'%A, %B %d, %Y %H:%M:%S')
'%d%b%Y'
```

### Subset Time Components

We can extract month, year, and hours of datetime. 
```{r}
mydate = as.POSIXlt('2005-4-19 7:01:00')
names(mydate)
```

### Creating Regularly Spaced Time Series

We can create a regularly spaced time series in two different ways. 
```{r}
# Create dates as a Date class object starting from 2016-01-01
dates1 <- seq(as.Date("2016-01-01"), length = 5, by = "days") # by can take 'weeks', '2 weeks', 'hours, etc argument
dates2 <- as.Date("2016-01-01", format) + 0:4
dates1 == dates2
seq(as.Date('2000-6-1'),to=as.Date('2000-8-1'),by='2 weeks')
```

### Time Zones

We can change the time zone of data.

Check the current time zone
```{r}
# check the system time zone
print(paste0('The currenet system\'s time zone is: ', Sys.timezone()))
```

```{r}
# What are the time
print(paste0('There are ', as.character(length(OlsonNames())), ' recognized Olson\'s time zone names'))
```

Change the timezone
```{r}
# set the timezone to `"America/Los_Angeles"`
as.POSIXct('2010-10-01 12:12:00', tz = 'America/Los_Angeles')
```

### Period between Two Times

We can calculate the period between the two datetimes. We can use this to create a new feature. For example, to predict sales during holidays, we can calculate a variable that measures distance from holiday.
```{r}
b1 = ISOdate(1977,7,13)
b2 = ISOdate(2003,8,14)
b2 - b1
difftime(b2,b1,units='weeks')
```

## Lubridate 

While default base R provides great functionalities, it is not as flexible and easy to use as lubridate. In most cases, for parsing and other purposes, I opt for lubridate package. 

The strengths of lubridate library:

- Lubridates allow us to easily create new columns on date or time components. 
- It also allows us to parse non-standard date formats
- It allows us to subset dates using rounding
- combine individual date component
- Can work with base R and xts/zoo as well.

### Parsing weird orders: 

```{r}
# Specify an order string to parse x
x <- "Monday June 1st 2010 at 4pm"
parse_date_time(x, orders = "amdy Ip")

# Specify order to include both "mdy" and "dmy"
two_orders <- c("October 7, 2001", "October 13, 2002", "April 13, 2003", 
  "17 April 2005", "23 April 2017")
parse_date_time(two_orders, orders = c('mdy','dmy'))

# Specify order to include "dOmY", "OmY" and "Y"
short_dates <- c("11 December 1282", "May 1372", "1253")
parse_date_time(short_dates, orders = c('d0mY', '0mY', 'Y'))
```

### Combining Time Components

```{r}
# Use make_date() to combine year, month and mday 
akl_hourly  <- akl_hourly_raw  %>% 
  mutate(date = make_date(year = year, month = month, day = mday))

# Parse datetime_string 
akl_hourly <- akl_hourly  %>% 
  mutate(
    datetime_string = paste(date, time, sep = "T"),
    datetime = ymd_hms(datetime_string)
  )

# Print date, time and datetime columns of akl_hourly
akl_hourly %>% select(date, time, datetime)

# Plot to check work
ggplot(akl_hourly, aes(x = datetime, y = temperature)) +
  geom_line()

```

### Extract time components

```{r}
# Extract the month of releases 
month(release_time) %>% table()

# Extract the year of releases
year(release_time) %>% table()

# How often is the hour before 12 (noon)?
mean(hour(release_time) < 12)

# How often is the release in am?
mean(am(release_time))
```

### Labeling weekdays

```{r}
# Use wday() to tabulate release by day of the week
wday(releases$datetime) %>% table()

# Add label = TRUE to make table more readable
wday(releases$datetime, label=TRUE) %>% table()

# Create column wday to hold labelled week days
releases$wday <- wday(releases$datetime, label=TRUE)

# Plot barchart of weekday by type of release
ggplot(releases, aes(x = wday)) +
  geom_bar() +
  facet_wrap(~ type, ncol = 1, scale = "free_y")
```

### Extracting for Filtering and Summarizing
```{r}
# Add columns for year, yday and month
akl_daily <- akl_daily %>%
  mutate(
    year = year(date),
    yday = yday(date),
    month = month(date, label = TRUE))

# Plot max_temp by yday for all years
ggplot(akl_daily, aes(x = yday, y = max_temp)) +
  geom_line(aes(group = year), alpha = 0.5)

# Examine distribution of max_temp by month
ggplot(akl_daily, aes(x = max_temp, y = month, height = ..density..)) +
  geom_density_ridges(stat = "density")

# Create new columns hour, month and rainy
akl_hourly <- akl_hourly %>%
  mutate(
    hour = hour(datetime),
    month = month(datetime, label = TRUE),
    rainy = weather == "Precipitation"
  )

# Filter for hours between 8am and 10pm (inclusive)
akl_day <- akl_hourly %>% 
  filter(hour >= 8, hour <= 22)

# Summarise for each date if there is any rain
rainy_days <- akl_day %>% 
  group_by(month, date) %>%
  summarise(
    any_rain = any(rainy)
  )

# Summarise for each month, the number of days with rain
rainy_days %>% 
  summarise(
    days_rainy = sum(any_rain)
  )
```

#### Date Rounding

When is rounding useful? In a lot of the same situations extracting date components is useful. The advantage of rounding over extracting is that it maintains the context of the unit. For example, extracting the hour gives you the hour the datetime occurred, but you lose the day that hour occurred on (unless you extract that too), on the other hand, rounding to the nearest hour maintains the day, month and year.

All three take a unit argument which specifies the resolution of rounding. You can specify "second", "minute", "hour", "day", "week", "month", "bimonth", "quarter", "halfyear", or "year". Or, you can specify any multiple of those units, e.g. "5 years", "3 minutes" etc.
```{r}
day1 <- ymd_hms("2016-05-03 07:13:28 UTC")

# Round down to day
floor_date(day1, unit = 'day')

# Round to nearest 5 minutes
round_date(day1, unit = '5 minutes')

# Round up to week 
ceiling_date(day1, unit = 'week')

# Subtract day1 rounded down to day
day1 - floor_date(day1, unit = 'day')

# Create day_hour, datetime rounded down to hour
data <- data %>%
  mutate(
    day_hour = floor_date(datetime, unit = 'hour')
  )

# Count observations per hour  
data %>% 
  count(day_hour) 

# Find day_hours with n != 2  
data %>% 
  count(day_hour) %>%
  filter(n != 2) %>% 
  arrange(desc(n))
```

#### Difference in Times

lubridate functions today() and now() which when called with no arguments return the current date and time in your system's timezone.

To get finer control over a difference between datetimes use the base function difftime(). For example instead of time1 - time2, you use difftime(time1, time2).

difftime() takes an argument units which specifies the units for the difference. Your options are "secs", "mins", "hours", "days", or "weeks".
```{r}
# The date of landing and moment of step
date_landing <- mdy("July 20, 1969")
moment_step <- mdy_hms("July 20, 1969, 02:56:15", tz = "UTC")

# How many days since the first man on the moon?
difftime(today(), date_landing, units = 'days')

# How many seconds since the first man on the moon?
difftime(now(), moment_step, units = 'secs')

# Three dates
mar_11 <- ymd_hms("2017-03-11 12:00:00", 
  tz = "America/Los_Angeles")
mar_12 <- ymd_hms("2017-03-12 12:00:00", 
  tz = "America/Los_Angeles")
mar_13 <- ymd_hms("2017-03-13 12:00:00", 
  tz = "America/Los_Angeles")

# Difference between mar_13 and mar_12 in seconds
difftime(mar_13, mar_12, units = 'secs')

# Difference between mar_12 and mar_11 in seconds
difftime(mar_12, mar_11, units = 'secs')
```

#### Time Spans: Periods and Durations

Timespans are hard because they dont have constant meaning. 

Define two time spans: 
1. period: human concept of a time span, datetime + period of one day = same time on the next date, variable length
2. duration: stopwatch concept of a time span, datetime + duration of one day = datetime + 86400 seconds, fixed number of seconds

period:
period constructors

duration:
duration constructors, always prefaced with d
all written in plural

A common use of time spans is to add or subtract them from a moment in time. 
```{r}
# Add a period of one week to mon_2pm
mon_2pm <- dmy_hm("27 Aug 2018 14:00")
mon_2pm + weeks(1)

# Add a duration of 81 hours to tue_9am
tue_9am <- dmy_hm("28 Aug 2018 9:00")
tue_9am + dhours(81)

# Subtract a period of five years from today()
today() - years(5)

# Subtract a duration of five years from today()
today() - dyears(5)
```

 Why did subtracting a duration of five years from today, give a different answer to subtracting a period of five years? Periods know about leap years, and since five years ago includes at least one leap year (assuming you aren't taking this course in 2100) the period of five years is longer than the duration of 365*5 days.

You can add and subtract timespans to create different length timespans, and even multiply them by numbers. For example, to create a duration of three days and three hours you could do: ddays(3) + dhours(3), or 3*ddays(1) + 3dhours(1) or even 3(ddays(1) + dhours(1)).
```{r}
# Time of North American Eclipse 2017
eclipse_2017 <- ymd_hms("2017-08-21 18:26:40")

# Duration of 29 days, 12 hours, 44 mins and 3 secs
synodic <- ddays(29) + dhours(12) + dminutes(44) + dseconds(3)

# 223 synodic months
saros <- 223*synodic

# Add saros to eclipse_2017
eclipse_2017 + saros
```

Generating sequences of datetimes
By combining addition and multiplication with sequences you can generate sequences of datetimes. For example, you can generate a sequence of periods from 1 day up to 10 days with,

1:10 * days(1)
Then by adding this sequence to a specific datetime, you can construct a sequence of datetimes from 1 day up to 10 days into the future

today() + 1:10 * days(1)
```{r}
# Add a period of 8 hours to today
today_8am <- today() + hours(8)

# Sequence of two weeks from 1 to 26
every_two_weeks <- 1:26*weeks(2)

# Create datetime for every two weeks for a year
today_8am + every_two_weeks

```

The tricky thing about months
What should ymd("2018-01-31") + months(1) return? Should it be 30, 31 or 28 days in the future? Try it. In general lubridate returns the same day of the month in the next month, but since the 31st of February doesn't exist lubridate returns a missing value, NA.

There are alternative addition and subtraction operators: %m+% and %m-% that have different behavior. Rather than returning an NA for a non-existent date, they roll back to the last existing date.
```{r}
# A sequence of 1 to 12 periods of 1 month
month_seq <- 1:12*months(1)

# Add 1 to 12 months to jan_31
jan_31 + month_seq

# Replace + with %m+%
jan_31 %m+% month_seq

# Replace + with %m-%
jan_31 %m-% month_seq
```

But use these operators with caution, unlike + and -, you might not get x back from x %m+% months(1) %m-% months(1). If you'd prefer that the date was rolled forward check out add_with_rollback() which has roll_to_first argument

#### Intervals

You can create an interval by using the operator %--% with two datetimes. For example ymd("2001-01-01") %--% ymd("2001-12-31") creates an interval for the year of 2001.

Once you have an interval you can find out certain properties like its start, end and length with int_start(), int_end() and int_length() respectively
```{r}
# Create an interval for reign
monarchs <- monarchs %>%
  mutate(reign = from %--% to) 

# Find the length of reign, and arrange
monarchs %>%
  mutate(length = int_length(reign)) %>% 
  arrange(desc(length)) %>%
  select(name, length, dominion)

```

Comparing intervals and datetimes
A common task with intervals is to ask if a certain time is inside the interval or whether it overlaps with another interval.

The operator %within% tests if the datetime (or interval) on the left hand side is within the interval of the right hand side. For example, if y2001 is the interval covering the year 2001,

y2001 <- ymd("2001-01-01") %--% ymd("2001-12-31")
Then ymd("2001-03-30") %within% y2001 will return TRUE and ymd("2002-03-30") %within% y2001 will return FALSE.

int_overlaps() performs a similar test, but will return true if two intervals overlap at all.
```{r}
# Print halleys
print(halleys)

# New column for interval from start to end date
halleys <- halleys %>% 
  mutate(visible = start_date %--% end_date)

# The visitation of 1066
halleys_1066 <- halleys[14, ] 

# Monarchs in power on perihelion date
monarchs %>% 
  filter(halleys_1066$perihelion_date %within% reign) %>%
  select(name, from, to, dominion)

# Monarchs whose reign overlaps visible time
monarchs %>% 
  filter(int_overlaps(halleys_1066$visible, reign)) %>%
  select(name, from, to, dominion)
```

Converting to durations and periods
Intervals are the most specific way to represent a span of time since they retain information about the exact start and end moments. They can be converted to periods and durations exactly: it's possible to calculate both the exact number of seconds elapsed between the start and end date, as well as the perceived change in clock time.

To do so you use the as.period(), and as.duration() functions, parsing in an interval as the only argument.
```{r}
# New columns for duration and period
monarchs <- monarchs %>%
  mutate(
    duration = as.duration(reign),
    period = as.period(reign)) 
    
# Examine results    
monarchs %>%
  select(c(name, duration, period))
```

#### Time Zone

Setting the timezone
If you import a datetime and it has the wrong timezone, you can set it with force_tz(). Pass in the datetime as the first argument and the appropriate timezone to the tzone argument. Remember the timezone needs to be one from OlsonNames().
```{r}
# Game2: CAN vs NZL in Edmonton
game2 <- mdy_hm("June 11 2015 19:00")

# Game3: CHN vs NZL in Winnipeg
game3 <- mdy_hm("June 15 2015 18:30")

# Set the timezone to "America/Edmonton"
game2_local <- force_tz(game2, tzone = "America/Edmonton")
game2_local

# Set the timezone to "America/Winnipeg"
game3_local <- force_tz(game3, tzone = "America/Winnipeg")
game3_local

# How long does the team have to rest?
as.period(game2_local %--% game3_local)
```

Viewing in a timezone
To view a datetime in another timezone use with_tz(). The syntax of with_tz() is the same as force_tz(), passing a datetime and set the tzone argument to the desired timezone. Unlike force_tz(), with_tz() isn't changing the underlying moment of time, just how it is displayed.

For example, the difference between now() displayed in the "America/Los_Angeles" timezone and "Pacific/Auckland" timezone is 0:

now <- now()
with_tz(now, "America/Los_Angeles") - with_tz(now,  "Pacific/Auckland")
```{r}
# What time is game2_local in NZ?
with_tz(game2_local, tzone = 'Pacific/Auckland')

# What time is game2_local in Corvallis, Oregon?
with_tz(game2_local, tzone = 'America/Los_Angeles')

# What time is game3_local in NZ?
with_tz(game3_local, tzone = 'Pacific/Auckland')
```

sometimes you just have a time without a date.

If you find yourself in this situation, the hms package provides an hms class of object for holding times without dates, and the best place to start would be with as.hms().
```{r}
as.hms()

# Import auckland hourly data 
akl_hourly <- read_csv("akl_weather_hourly_2016.csv")

# Examine structure of time column
str(akl_hourly$time)

# Examine head of time column
head(akl_hourly$time)

# A plot using just time
ggplot(akl_hourly, aes(x = time, y = temperature)) +
  geom_line(aes(group = make_date(year, month, mday)), alpha = 0.2)
```

#### Fast Parsing

Fast parsing with fasttime
The fasttime package provides a single function fastPOSIXct(), designed to read in datetimes formatted according to ISO 8601. Because it only reads in one format, and doesn't have to guess a format, it is really fast!
```{r}
# Use fastPOSIXct() to parse dates
fastPOSIXct(dates)
```

Fast parsing with lubridate::fast_strptime
lubridate provides its own fast datetime parser: fast_strptime(). Instead of taking an order argument like parse_date_time() it takes a format argument and the format must comply with the strptime() style.
It means any character that represents a datetime component must be prefixed with a % and any non-whitespace characters must be explicitly included.
```{r}
# Head of dates
head(dates)

# Parse dates with fast_strptime
fast_strptime(dates, 
    format = '%Y-%m-%dT%H:%M:%SZ') %>% str()

# Comparse speed to ymd_hms() and fasttime
microbenchmark(
  ymd_hms = ymd_hms(dates),
  fasttime = fastPOSIXct(dates),
  fast_strptime = fast_strptime(dates, 
    format = '%Y-%m-%dT%H:%M:%SZ'),
  times = 20)
```

#### Outputting Pretty Dateformats for Consumable

Outputting pretty dates and times
An easy way to output dates is to use the stamp() function in lubridate. stamp() takes a string which should be an example of how the date should be formatted, and returns a function that can be used to format dates.
```{r}
# Create a stamp based on "Saturday, Jan 1, 2000"
date_stamp <- stamp('Saturday, Jan 1, 2000')

# Print date_stamp
date_stamp

# Call date_stamp on today()
date_stamp(today())

# Create and call a stamp based on "12/31/1999" "MM/DD/YYYY"
stamp('12/31/1999')(today())

# Use string finished for stamp()
stamp(finished)(today())
```

### xts object

extensible time object

xts objects are matrix objects internally. xts objects are indexed by a formal time object. Most zoo methods work for xts.

The main xts constructor takes a number of arguments, but the two most important are x for the data and order.by for the index. x must be a vector or matrix. order.by is a vector which must be the same length or number of rows as x, be a proper time or date object (very important!), and be in increasing order.

xts also allows you to bind arbitrary key-value attributes to your data. This lets you keep metadata about your object inside your object. To add these at creation, you simply pass additional name = value arguments to the xts() function.
```{r}
y <- xts(x, order.by = index(x), frequency, unique, tzone)
coredata(y)
index(y)
as.xts() # ts -> xts
read.zoo()
```

# III. EDA and Visualization

Once we have cleaned data using R's libraries, we can inspect and explore patterns in data. 

The best tool is graph. Graphs enable us to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.

As mentioned earlier, R has two time series objects - ts and xts. Each object entails different functions. For more in-depth analysis on multivariate data, I use library(plotly) to explore greater details. 

There are four important aspects to consider. 

  1. Univariate Analysis - distributional analysis (histogram, boxplot)
  2. Multivariate Analysis - Multiple line charts and Correlation Matrix
  2. Trend and Seasonality Analysis
    - Seasonality plot
    - Subseasonality plots 
  3. Stationarity
  4. Decomposition

## xts Plotting

Correlation to your existing portfolio to assess diversification, return histogram to assess risk and box and whisker plot to assess average return

The plot.xts() function is the most useful tool in the R time series data visualization artillery. It is fairly similar to general plotting, but its x-axis contains a time scale. You can use plot() instead of plot.xts() if the object used in the function is an xts object.

```{r}
# Replot yahoo data with labels for X and Y axes
plot(data$yahoo, main = 'yahoo', xlab = 'date', ylab = 'price')
```

#### Adding more times series

lines() function is especially helpful when you want to modify an existing plot.
```{r}
# Replot with same title, add subtitle, use bars
plot(data$microsoft, main = 'microsoft', sub = 'Daily closing price since 2015', type = 'h')
# Change line color to red
lines(data$microsoft, col = 'red')
```

tailor the window layout using the par() function. To adjust the size of the margins and characters in the text, set the appropriate decimal value to to the options mex and cex, respectively. 
```{r}
# Replot with reduced margin and character sizes
par(mfrow = c(2,1), mex = 0.6, cex = 0.8) #mex margin sizes layout making the character size and the margin size 80% of their normal sizes
plot(data$yahoo, main = 'yahoo')
plot(data$microsoft, main = 'microsoft')
```

If you want a scale for this time series on the right side of the plot with equally spaced tick marks, use axis(side, at), where side is an integer specifying which side of the plot the axis should be drawn on, and at is set equal to pretty(mydata2). to distinguish these two time series, you can add a legend with the legend() function. Since there are two time series in the plot, some options in legend() are set to a vector of length two.
```{r}
# Plot the "microsoft" series
plot(data$microsoft, main = 'Stock prices since 2015')

# Add the "dow_chemical" series in red
lines(data$dow_chemical, col = 'red')

# Add a Y axis on the right side of the chart
axis(side = 4, at = pretty(data$dow_chemical))

# Add a legend in the bottom right corner
legend(x ='bottomright',
  legend = c('microsoft', 'dow_chemical'),
  col = c('black', 'red'),
  lty = c(1, 1))
```

#### Adjusting labels

```{r}
# Same plot 
plot(data$microsoft, main = "Dividend date and amount")
lines(data$citigroup, col = "orange", lwd = 2)
axis(side = 4, at = pretty(data$citigroup), col = "orange")

# Create the two legend strings
micro <- paste0("Microsoft div. of ", micro_div_value," on ", micro_div_date)
citi <- paste0("Citigroup div. of ", citi_div_value," on ", citi_div_date)

# Create the legend in the bottom right corner
legend(x = 'bottomright', legend = c(micro, citi), col = c('black', 'orange'), lty = c(1, 1))
```

#### Highlight specific time

use the function abline() to add straight lines through an existing plot. Specifically, you can draw a horizontal line to identify a particular date by setting h to a specific Y value, and a vertical line to identify a particular level by setting v to a specific X value. Recall that the index of an xts object are date objects, so the X values of a plot will also contain dates. 
```{r}
# Plot the "citigroup" time series
plot(data$citigroup, main = 'Citigroup')

# Create vert_line to identify January 4th, 2016 in citigroup
vert_line <- which(index(data$citigroup) == as.Date('2016-01-04'))

# Add a red vertical line using vert_line
abline(v = .index(data$citigroup)[vert_line], col = "red")

# Create hori_line to identify average price of citigroup
hori_line <- mean(data$citigroup)

# Add a blue horizontal line using hori_line
abline(h = hori_line, col = "blue")
```

#### Highlight of subperiod

To highlight a specific period in a time series, you can display it in the plot in a different background color. The chart.TimeSeries() function in the PerformanceAnalytics package offers a very easy and flexible way of doing this.
```{r}
# Create period to hold the 3 months of 2015
period <- c('2015-01/2015-03')

# Highlight the first three months of 2015 
chart.TimeSeries(data$citigroup, period.areas = period)

# Highlight the first three months of 2015 in light grey
chart.TimeSeries(data$citigroup, period.areas= period, period.color = 'lightgrey')
```

### Multiple Time Series

Grouped barchart: for a single period, there are as many bars as time series
Stacked bar chart: for each period, there is a single bar, and each time series is represented by a portion of the bar proportional to the value of the time series at this date (i.e. the total at each period adds up to 100%)
```{r}
# Plot stacked barplot
barplot(height = portfolio[,c('stocka','stockb')])

# Plot grouped barplot
barplot(portfolio[,c('stocka','stockb')], beside = T)
```

If you want to go even further than simply plotting variables and instead investigate whether any relationship exists between 2 variables, you can draw a scatterplot. This is a graph where the values of two variables are plotted along two axes.

The pattern of the resulting points is used to reveal the presence of any correlation; usually, a regression line is added to identify the tendency, if there is any:

An upward sloping regression line indicates a positive linear relationship between A and B (when A goes up B tends to goes up as well)
A downward sloping regression line indicates a negative linear relationship between A and B
```{r}
# Draw the scatterplot
plot(sp500, citi)

# Draw a regression line
abline(reg=lm(citi ~ sp500), col='red', lwd=2)
```

### Correlation matrix

What if you want to evaluate the relationship between mutiple time series? The most common tool to use is a correlation matrix, which is a table showing correlation coefficients between pairs of variables. Several types of correlations exist but the most used ones are:

Pearson correlation: measures the linear relationship between 2 variables
Spearman rank correlation: measures the statistical dependency between the ranking of 2 variables (not necessarily linear)
The latter is used when there is no assumption made on the distribution of the data. All this is achieved in R using the function cor(). You can use the method argument to select the desired correlation type. "pearson" is the default method, but you can specify "spearman" as well.
```{r}
# Create correlation matrix using Pearson method
cor(my_data, method = 'pearson')

# Create correlation matrix using Spearman method
cor(my_data, method = 'spearman')
```

It's also possible to have a graphical representation of those relationships using scatterplots.

Specifically, the relationship between pairs() of time series is represented by a facetted scatterplot of all pairs at once. This is very convenient for a quick comparison betwen pairs of time series.

When you have a small number of time series to compare, a scatterplot matrix can be useful to visualize everything at once.
```{r}
# Create scatterplot matrix
pairs(my_data)

# Create upper panel scatterplot matrix
pairs(my_data, lower.panel = NULL)
```

R offers other ways of displaying the correlation matrix. With the corrplot package, the visualization of correlations is made easier and more powerful by allowing you to represent the correlations with numbers, symbols, colors, and more.

### Corrplot

corrplot() function to draw some correlation charts.
"circle", "square", "ellipse", "number", "shade", "color", "pie"
```{r}
# Create correlation matrix
corrplot(cor_mat)

# Create correlation matrix with numbers
corrplot(cor_mat, method = 'number')

# Create correlation matrix with colors
corrplot(cor_mat, method = 'color')

# Create upper triangle correlation matrix
corrplot(cor_mat, method = 'number', type = 'upper')
```

Heatmap
deal with many multiple time series like 100 of them
heatmap is a representation of data in the form of a map or diagram in which values are represented as colors

Should you want to check correlations betweens hundreds of time series, representing correlations with numbers is not really helpful - for a dataset of 100 elements, you would have to analyze 10,000 (100 x 100) correlation numbers!

In this case, a heatmap is a better suited tool. A heatmap is a map or diagram in which data values are represented as colors. When using one, it might also be useful to reorder the corelation matrix to make it more readable. You can create heatmaps using corrplot(method = "color").
```{r}
# Draw the upper heatmap with hclust
corrplot(cor_mat, method = 'color', type = 'upper', order = 'hclust')
```

## Performance Analytics

```{r}
# Draw value, return, drawdowns of old portfolio
charts.PerformanceSummary(old.vs.new.portfolio$old.portfolio.rtn)

# Draw value, return, drawdowns of new portfolio
charts.PerformanceSummary(old.vs.new.portfolio$new.portfolio.rtn)

# Draw both portfolios on same chart
charts.PerformanceSummary(old.vs.new.portfolio[,c('old.portfolio.rtn', 'new.portfolio.rtn')])
```

## ts object

#### Missing Data

```{r}
gold2 <- na.interp(gold)
autoplot(gold2, series="Interpolated") +
  autolayer(gold, series="Original") +
  scale_colour_manual(
    values=c(`Interpolated`="red",`Original`="gray"))
```

#### Outlier

```{r}
tsoutliers(gold)
gold %>%
  tsclean() %>%
  ets() %>%
  forecast(h=50) %>%
  autoplot()
```

### Univariate Time Series

We need to learn three characteristics of time series: location, dispersion, and distribution. 

The very first step in the analysis of any time series is to address if the time series have the right mathematical properties to apply the standard statistical framework. If not, you must transform the time series first.

In finance, price series are often transformed to differenced data, making it a return series. In R, the ROC() (which stands for "Rate of Change") function from the TTR package does this automatically to a price or volume series x
```{r}
# Plot Apple's stock price 
plot(data, main = "Apple stock price")

# Create a time series called rtn
rtn <- ROC(data)

# Plot Apple daily price and daily returns 
par(mfrow=c(1,2))
plot(data)
plot(rtn)
```

distribution of time series: histogram and boxplot

help detect outliers. in finance, this indicates riskiness of stock.

A simple chart of returns does not reveal much about the time series properties; often, data must be displayed in a different format to visualize interesting features.

The density function, represented by the histogram of returns, indicates the most common returns in a time series without taking time into account. In R, these are calculated with the hist() and density() functions.
```{r}
# Create a histogram of Apple stock returns
hist(rtn, main = 'Apple stock return distribution', probability = TRUE)

# Add a density line
lines(density(rtn))

# Redraw a thicker, red density line
lines(density(rtn), col = 'red', lwd = 2)
```

```{r}
# Plot new and old portfolio values on same chart
plot(old.vs.new.portfolio$old.portfolio.value)
lines(old.vs.new.portfolio$new.portfolio.value, col = "red")

# Plot density of the new and old portfolio returns on same chart
plot(density(old.vs.new.portfolio$old.portfolio.rtn))
lines(density(old.vs.new.portfolio$new.portfolio.rtn), col ="red")
```

A box and whisker plot gives information regarding the shape, variability, and center (or median) of a data set. It is particularly useful for displaying skewed data.

By comparing the data set to a standard normal distribution, you can identify departure from normality (asymmetry, skewness, etc). The lines extending parallel from the boxes are known as whiskers, which are used to indicate variability outside the upper and lower quartiles, i.e. outliers. Those outliers are usually plotted as individual dots that are in-line with whiskers.
```{r}
# Redraw both plots on the same graphical window
par(mfrow=c(2,1))
boxplot(rtn, horizontal = T)
boxplot(rnorm(1000), horizontal = T)
```

## ggplot2

We can always use ggplot2 to make lineplots. 

We can use scale_x_Date to adjust axis scale.
```{r}
# Examine histograms of downloads by version
ggplot(logs, aes(x = datetime)) +
  geom_histogram() +
  geom_vline(aes(xintercept = as.numeric(release_time)))+
  facet_wrap(~ r_version, ncol = 1)

xlim(as.Date(date1), as.Date(date2))
scale_x_Date(date_breaks = '10 years', date_labels = '%Y')
```

## autoplot()
use the autoplot() function to produce a time plot of the data with or without facets, or panels that display different subsets of data

which.max() can be used to identify the smallest index of the maximum value
To find the number of observations per unit time, use frequency()

Apply the frequency() function to each commodity to get the number of observations per unit time. This would return 52 for weekly data, for example.
```{r}
# Plot the data with facetting
autoplot( myts, facets = T)

# Plot the data without facetting
autoplot(myts, facets = F)

# Plot the three series
autoplot(gold)
autoplot(woolyrnq)
autoplot(gas)

# Find the outlier in the gold series
goldoutlier <- which.max(gold)

# Look at the seasonal frequencies of the three series
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```

One way of splitting a time series is by using the window() function, which extracts a subset from the object x observed between the times start and end.

### Trend, Seasonal, Cyclic

Trend: a pattern exists involving a long term increase or decrease in the data

seasonal: a periodic pattern exists due too the calendar (e.g. the quarter, month, or day of the week)

Cyclic: a pattern exists where the data exhibis rises and falls that are not of fixed period (duration of at least 2 years). Buiness cycle might last 3, 5, 8 years between peaks or troughs. In contrast to seasonal pattern which os of the same length always. 

Where is volatility? trough at the end of period or at the beginning?

Annual data cannot be seasonal; length of cycles can vary

We should make sure seasons and cycles are different because very different models are used

#### Seasonal Plot and Subseason plot

A seasonal plot is similar to a time plot except that the data are plotted against the individual "seasons" in which the data were observed. You can create one using the ggseasonplot() function the same way you do with autoplot().

An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal; to make one, simply add a polar argument and set it to TRUE.

A subseries plot comprises mini time plots for each season. Here, the mean for each season is shown as a blue horizontal line.
```{r}
# Create plots of the a10 data
autoplot(a10)
ggseasonplot(a10)

# Produce a polar coordinate season plot for the a10 data
ggseasonplot(a10, polar = T)

# Restrict the ausbeer data to start in 1992
beer <- window(ausbeer, start = 1992)

# Make plots of the beer data
autoplot(beer)
ggsubseriesplot(beer)
```

#### Lag plot

plot each observation against another observation that occurred some time previously by using gglagplot(). This is called a lag plot because you are plotting the time series against lags of itself.
```{r}
# Create a lag plot of the oil data
gglagplot(oil)
```

Look at how the relationships change as the lag increases.

Autocorrelogram: blue horizontal lines represent positive and negative significance thresholds; autocorrelation is important because it rells you the relationship between observations

Another important piece of information is the relationship between one point in the time series and points that come before it. This is called autocorrelation and it can be displayed as a chart which indicates the correlation between points separated by various time lags.

In R, you can plot the autocorrelation function using acf(), which by default, displays the first 30 lags (i.e. the correlation between points n and n - 1, n and n - 2, n and n - 3 and so on up to 30). The autocorrelogram, or the autocorrelation chart, tells you how any point in the time series is related to its past as well as how significant this relationship is. The significance levels are given by 2 horizontal lines above and below 0.

If ACF is positive, then positive return is followed by positive, and negativereturn is followed by negative. 
```{r}
# Redraw with a maximum lag of 10
acf(rtn, main = 'Apple return autocorrelation', lag.max = 10)
```

A q-q plot is a plot of the quantiles of one dataset against the quantiles of a second dataset. This is often used to understand if the data matches the standard statistical framework, or a normal distribution.

If the data is normally distributed, the points in the q-q plot follow a straight diagonal line. This is useful to check for normality at a glance but note that it is not an accurate statistical test. In the video, you saw how to create a q-q plot using the qqnorm() function, and how to create a reference line for if the data were perfectly normally distributed with qqline()
```{r}
# Create q-q plot
qqnorm(rtn, main = 'Apple return QQ-plot')

# Add a red line showing normality
qqline(rtn, col = 'red')
```

## Stationarity

Stationary series is stable:

  - mean is constant over time (no trend)
  - correlation structure (variability) remains constant over time (e.g. first half and second half should look the same); relationship between contiguous values remains the same 
  
benefit of stationarity: easier to estimate; we can first estimate by averaging since mean is constant and pairs can be used to estimate correlation on different lags 

global temperature time series (random walk: current value is previous value plus error) can be made stationary by first differencing; $X_{t} = X_{t-1} + W_{t}$ where $W_{t}$
random walk with drift is where a constant is added to random walk
price of chicken: stationary around trend (trend stationary): differencing; $Y = a + \beta t + X$
nonstationarity in trend and seasonality: first log and then difference

### Autocorrelogram

The correlations associated with the lag plots form what is called the autocorrelation function (ACF). The ggAcf() function produces ACF plots. Look at the ACF plot for sunspot.year and find the maximum ACF (y), or the tallest bar on the grpah.
```{r}
# Create an ACF plot of the oil data
ggAcf(oil)
```

When data are either seasonal or cyclic, the ACF will peak around the seasonal lags or at the average cycle length.

### Testing Autocorrelation on all periods altogether: Ljung-Box test

Ljung box test considers the first h autocorrelation values altogether. A significant test (small p-value) indicates the data are probably not white noise.
```{r}
# Ljung-Box test of the differenced series
Box.test(goog, lag = 10, type = "Ljung")
```

### Differencing

 This shows one way to make a non-stationary time series stationary - compute the differences between consecutive observations. This is known as differencing.

#### First Differencing

Transformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.

As well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for non-stationary data, the value of r1is often large and positive.
```{r}
# Plot the original series
autoplot(goog)

# Plot the differenced series
autoplot(diff(goog))

# ACF of the differenced series
ggAcf(diff(goog))


```

differencing both your trend stationary data and your random walk data has the effect of removing the trend
```{r}
plot(diff(x))
```

## Time series with trend and heteroskedasticity
Example: time series with percentage growth each period like savings $X_{t} = (1+p_{t})X_{t-1}$. Log differencing can create $log X_{t} - log X_{t-1} = p_{t}$
```{r}
plot(diff(log(x)))
```

A stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary - the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary - it does not matter when you observe it, it should look much the same at any point in time. Some cases can be confusing - a time series with cyclic behaviour (but with no trend or seasonality) is stationary. This is because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be. In general, a stationary time series will have no predictable patterns in the long-term. Time plots will show the series to be roughly horizontal (although some cyclic behavior is possible), with constant variance.
```{r}
Box.test(diff(goog200), lag=10, type="Ljung-Box")
```

## Non-seasonal differencing for stationarity
Differencing is a way of making a time series stationary; this means that you remove any systematic patterns such as trend and seasonality from the data. A white noise series is considered a special case of a stationary time series.

With non-seasonal data, you use lag-1 differences to model changes between observations rather than the observations directly. 
```{r}
# Plot the differenced murder rate
autoplot(diff(wmurders))

# Plot the ACF of the differenced murder rate
ggAcf(diff(wmurders))
```

## Seasonal differencing for stationarity
With seasonal data, differences are often taken between observations in the same season of consecutive years, rather than in consecutive periods. For example, with quarterly data, one would take the difference between Q1 in one year and Q1 in the previous year. This is called seasonal differencing.

Sometimes you need to apply both seasonal differences and lag-1 differences to the same series, thus, calculating the differences in the differences.
```{r}
# Plot the data
autoplot(h02)

# Take logs and seasonal differences of h02
difflogh02 <- diff(log(h02), lag = 12)

# Plot difflogh02
autoplot(difflogh02)

# Take another difference and plot
ddifflogh02 <- diff(difflogh02)
autoplot(ddifflogh02)

# Plot ACF of ddifflogh02
ggAcf(ddifflogh02)
```

#### Seasonal Differencing

A seasonal difference is the difference between an observation and the previous observation from the same season.
```{r}
cbind("Sales ($million)" = a10,
      "Monthly log sales" = log(a10),
      "Annual change in log sales" = diff(log(a10),12)) %>%
  autoplot(facets=TRUE) +
    xlab("Year") + ylab("") +
    ggtitle("Antidiabetic drug sales")
```

#### Both

When both seasonal and first differences are applied, it makes no difference which is done first-the result will be the same. However, if the data have a strong seasonal pattern, we recommend that seasonal differencing be done first, because the resulting series will sometimes be stationary and there will be no need for a further first difference. If first differencing is done first, there will still be seasonality present.
```{r}
cbind("Billion kWh" = usmelec,
      "Logs" = log(usmelec),
      "Seasonally\n differenced logs" =
        diff(log(usmelec),12),
      "Doubly\n differenced logs" =
        diff(diff(log(usmelec),12),1)) %>%
  autoplot(facets=TRUE) +
    xlab("Year") + ylab("") +
    ggtitle("Monthly US net electricity generation")
```

### Log Differencing

When variance is increasing -> log (variance stabilizing transformation)
When there is trend, difference
When there is seasonal persistence with e.g. three cycles per year, then take a seasonal difference

```{r}
# Plot unemp 
plot(unemp)

# Difference your data and plot it
d_unemp <- diff(unemp)
plot(d_unemp)

# Seasonally difference d_unemp and plot it
dd_unemp <- diff(d_unemp, lag = 12)  
plot(dd_unemp)
```

### Do we need differencing?

One way to determine more objectively whether differencing is required is to use a unit root test. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required.
A number of unit root tests are available, which are based on different assumptions and may lead to conflicting answers. In our analysis, we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski, Phillips, Schmidt, & Shin. In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05) suggest that differencing is required.

This process of using a sequence of KPSS tests to determine the appropriate number of first differences is carried out by the function ndiffs().

A similar function for determining whether seasonal differencing is required is nsdiffs(), which uses the measure of seasonal strength
```{r}
goog %>% ur.kpss() %>% summary()
ndiffs(goog)
usmelec %>% log() %>% nsdiffs()
```

## Decomposition of Time Series into Trend and Seasonality 

### Classical Decomposition: Additive and Mltiplicative
```{r}
elecequip %>% decompose(type="multiplicative") %>%
  autoplot() + xlab("Year") +
  ggtitle("Classical multiplicative decomposition
    of electrical equipment index")

```

### X11 Decomposition
Another popular method for decomposing quarterly and monthly data is the X11 method which originated in the US Census Bureau and Statistics Canada. The process is entirely automatic and tends to be highly robust to outliers and level shifts in the time series.

```{r}
library(seasonal)
elecequip %>% seas(x11="") -> fit
autoplot(fit) +
  ggtitle("X11 decomposition of electrical equipment index")

```

```{r}
autoplot(elecequip, series="Data") +
  autolayer(trendcycle(fit), series="Trend") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") + ylab("New orders index") +
  ggtitle("Electrical equipment manufacturing (Euro area)") +
  scale_colour_manual(values=c("gray","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))
```

```{r}
fit %>% seasonal() %>% ggsubseriesplot() + ylab("Seasonal")
```

### SEATS

"SEATS" stands for "Seasonal Extraction in ARIMA Time Series" This procedure was developed at the Bank of Spain, and is now widely used by government agencies around the world. The procedure works only with quarterly and monthly data. So seasonality of other kinds, such as daily data, or hourly data, or weekly data, require an alternative approach.
```{r}
library(seasonal)
elecequip %>% seas() %>%
autoplot() +
  ggtitle("SEATS decomposition of electrical equipment index")
```

### STL

STL is a versatile and robust method for decomposing time series. STL is an acronym for "Seasonal and Trend decomposition using Loess", while Loess is a method for estimating nonlinear relationships. The STL method was developed by Cleveland, Cleveland, McRae, & Terpenning

Unlike SEATS and X11, STL will handle any type of seasonality, not only monthly and quarterly data.

The seasonal component is allowed to change over time, and the rate of change can be controlled by the user.

The smoothness of the trend-cycle can also be controlled by the user.

It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component. On the other hand, STL has some disadvantages. In particular, it does not handle trading day or calendar variation automatically, and it only provides facilities for additive decompositions.
```{r}
elecequip %>%
  stl(t.window=13, s.window="periodic", robust=TRUE) %>%
  autoplot()
```

The two main parameters to be chosen when using STL are the trend-cycle window (t.window) and the seasonal window (s.window). These control how rapidly the trend-cycle and seasonal components can change. Smaller values allow for more rapid changes. Both t.window and s.window should be odd numbers; t.window is the number of consecutive observations to be used when estimating the trend-cycle; s.window is the number of consecutive years to be used in estimating each value in the seasonal component. The user must specify s.window as there is no default. Setting it to be infinite is equivalent to forcing the seasonal component to be periodic (i.e., identical across years). Specifying t.window is optional, and a default value will be used if it is omitted.

The mstl() function provides a convenient automated STL decomposition using s.window=13, and t.window also chosen automatically. This usually gives a good balance between overfitting the seasonality and allowing it to slowly change over time. But, as with any automated procedure, the default settings will need adjusting for some time series.

As with the other decomposition methods discussed in this book, to obtain the separate components plotted in Figure 6.8, use the seasonal() function for the seasonal component, the trendcycle() function for trend-cycle component, and the remainder() function for the remainder component. The seasadj() function can be used to compute the seasonally adjusted series.

### Multiple Seasonality

```{r}
calls %>% mstl() %>%
  autoplot() + xlab("Week")
```

# IV. Feature Engineering:

## Creating New Features 

We can create time-related features such as holiday or weekends using lubridate. 

1. The number of hours and days from certain event
- how far observation is from Christmas
```{r}

```

2. Components of Date such as weekday vs weekend given time data
```{r}

```

3. Time interval overlaps?
```{r}

```

4. Rounding Datetime to just month or week
```{r}

```

## Variance Stabilization Transform

Box-Cox transform
You want to find a value of $\lambda$ that makes the seasonal fluctuations of roughly the same size across the series.
```{r}
# Try lambda in Box-Cox transformations
a10 %>% BoxCox(lambda = 0.0) %>% autoplot()

# Compare with BoxCox.lambda()
BoxCox.lambda(a10)
```

Yeo-Johnson
```{r}

```

## Smoothing

```{r}
ma(elecsales, 5)

```
Simple moving averages such as these are usually of an odd order (e.g., 3, 5, 7, etc.). This is so they are symmetric
It is possible to apply a moving average to a moving average. One reason for doing this is to make an even-order moving average symmetric.
The most common use of centred moving averages is for estimating the trend-cycle from seasonal data. 

```{r}
autoplot(elecequip, series="Data") +
  autolayer(ma(elecequip, 12), series="12-MA") +
  xlab("Year") + ylab("New orders index") +
  ggtitle("Electrical equipment manufacturing (Euro area)") +
  scale_colour_manual(values=c("Data"="grey","12-MA"="red"),
                      breaks=c("Data","12-MA"))
```


## Differencing and Detrending

If we realize there is any trend, it is a good option to difference. 
```{r}

```

## Seasonal Adjustment

When we realize there is a seasonality, then we should use seasonal differencing.
```{r}

```

If the variation due to seasonality is not of primary interest, the seasonally adjusted series can be useful. For example, monthly unemployment data are usually seasonally adjusted in order to highlight variation due to the underlying state of the economy rather than the seasonal variation. An increase in unemployment due to school leavers seeking work is seasonal variation, while an increase in unemployment due to an economic recession is non-seasonal. Most economic analysts who study unemployment data are more interested in the non-seasonal variation. Consequently, employment data (and many other economic series) are usually seasonally adjusted.

Seasonally adjusted series contain the remainder component as well as the trend-cycle. Therefore, they are not "smooth", and "downturns" or "upturns" can be misleading. If the purpose is to look for turning points in a series, and interpret any changes in direction, then it is better to use the trend-cycle component rather than the seasonally adjusted data.

calendar adjustments, population adjustments, inflation adjustments and mathematical transformations

For example, if you are studying the monthly milk production on a farm, there will be variation between the months simply because of the different numbers of days in each month, in addition to the seasonal variation across the year.

```{r}
dframe <- cbind(Monthly = milk,
                DailyAverage = milk/monthdays(milk))
  autoplot(dframe, facet=TRUE) +
    xlab("Years") + ylab("Pounds") +
    ggtitle("Milk production per cow")

```

```{r}
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation") +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
```


# V. Models and Forecasting

Once objective is defined data cleaned, we can run models

Depending on the dataset we can run different models. 

- Univariate Time Series
 . Beer sales 

- Multiple Time Series
 . Istanbul stock exchange 
 . US GDP and Unemployment Rate
 . CO2 emission
 . Chicago Mercantile Exchange seat price and trading volume

- Hierarchicagl Time Series
 . Energy Usage (Kaggle Competition)
 . Walmart Sales (Kaggle Competition)
 
Model List

  1. Simple Models
    - Average
    - Naive
    - Seasonal Naive
    - Drift
  2. Linear Regression
  3. Cubic Spline
  4. Exponential Smoothing
    - Simple Exponential Smoothing
    - Holt and Damped
    - Holt Winter
    - State Soace and ETS 
  5. ARIMA
    - ARIMA
    - Dynamic ARIMA
    - SARIMA
    - ARFIMA
    - VARMA
  6. TBATS
  7. Hierarchical and Grouped Time Series 
  7. Neural Network
    - Neural Network
    - RNN

## A. Simple Models

### 1. Average method

forecasts of all future values are equal to the average (or "mean") of the historical data
meanf(), which gives forecasts equal to the mean of all observations.
```{r}
# y contains the time series
# h is the forecast horizon
# Create the training data as train
train <- subset(gold, end = 1000)

# Compute naive forecasts and save to naive_fc
naive_fc <- naive(train, h = 108)

# Compute mean forecasts and save to mean_fc
mean_fc <- meanf(train, h = 108)

# Use accuracy() to compute RMSE statistics
accuracy(naive_fc, gold)
accuracy(mean_fc, gold)

# Assign one of the two forecasts as bestforecasts
bestforecasts <- naive_fc
```

### 2. Naïve method

For naïve forecasts, we simply set all forecasts to be the value of the last observation.
```{r}
naive(y, h)
autoplot( )
rwf(y, h) # Equivalent alternative
```

### 3. Seasonal naïve method

A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year)
```{r}
snaive(y, h)
autoplot( )
```

```{r}
# Create three training series omitting the last 1, 2, and 3 years
train1 <- window(vn[, "Melbourne"], end = c(2014, 4))
train2 <- window(vn[, "Melbourne"], end = c(2013, 4))
train3 <- window(vn[, "Melbourne"], end = c(2012, 4))

# Produce forecasts using snaive()
fc1 <- snaive(train1, h = 4)
fc2 <- snaive(train2, h = 4)
fc3 <- snaive(train3, h = 4)

# Use accuracy() to compare the MAPE of each series
accuracy(fc1, vn[, "Melbourne"])["Test set", "MAPE"]
accuracy(fc2, vn[, "Melbourne"])["Test set", "MAPE"]
accuracy(fc3, vn[, "Melbourne"])["Test set", "MAPE"]
```

### 4. Drift method

A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data.
```{r}
rwf(y, h, drift=TRUE)
```

```{r}
# Set training data from 1992 to 2007
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
# Plot some forecasts
autoplot(beer2) +
  autolayer(meanf(beer2, h=11),
    series="Mean", PI=FALSE) +
  autolayer(naive(beer2, h=11),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(beer2, h=11),
    series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
    series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
    series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
    series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))

```
Sometimes one of these simple methods will be the best forecasting method available; but in many cases, these methods will serve as benchmarks rather than the method of choice. That is, any forecasting methods we develop will be compared to these simple methods to ensure that the new method is better than these simple alternatives. If not, the new method is not worth considering.

## B.linear regression

A trend variable can be specified in the tslm() function using the trend predictor. 
```{r}
tslm(Consumption ~ Income, data=uschange)
fit.consMR <- tslm(
  Consumption ~ Income + Production + Unemployment + Savings,
  data=uschange)
summary(fit.consMR)
```

```{r}
autoplot(uschange[,'Consumption'], series="Data") +
  autolayer(fitted(fit.consMR), series="Fitted") +
  xlab("Year") + ylab("") +
  ggtitle("Percent change in US consumption expenditure") +
  guides(colour=guide_legend(title=" "))
```

```{r}
cbind(Data = uschange[,"Consumption"],
      Fitted = fitted(fit.consMR)) %>%
  as.data.frame() %>%
  ggplot(aes(x=Data, y=Fitted)) +
    geom_point() +
    ylab("Fitted (predicted values)") +
    xlab("Data (actual values)") +
    ggtitle("Percent change in US consumption expenditure") +
    geom_abline(intercept=0, slope=1)
```

## C. Cubic Splines
```{r}
marathon %>%
  splinef(lambda=0) %>%
  autoplot()
marathon %>%
  splinef(lambda=0) %>%
  checkresiduals()
```

## D. Exponential Smoothing

Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation, the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in business.

The selection of the method is generally based on recognising key components of the time series (trend and seasonal) and the way in which these enter the smoothing method (e.g., in an additive, damped or multiplicative manner).

it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past - the smallest weights are associated with the oldest observations

alpha goes up -> naive 

component form and weighted average form

Choosing Optimal

### 1. Simple Exponential Smoothing

This method is suitable for forecasting data with no clear trend or seasonal pattern.

Need to estimate two parameters initial level l_{0} and alpha - smoothing parameter

nonlinear optimization of minimizng SSE. Large alpha means more weight on most recent observation and decay very quickly 

The ses() function produces forecasts obtained using simple exponential smoothing (SES). The parameters are estimated using least squares estimation. All you need to specify is the time series and the forecast horizon; the default forecast time is h = 10 years.

You will also use summary() and fitted(), along with autolayer() for the first time, which is like autoplot() but it adds a "layer" to a plot rather than creating a new plot.
```{r}
# Use ses() to forecast the next 10 years of winning times
fc <- ses(marathon, h = 10)

# Use summary() to see the model parameters
summary(fc)

# Use autoplot() to plot the forecasts
autoplot(fc)

# Add the one-step forecasts for the training data to the plot
autoplot(fc) + autolayer(fitted(fc))
```

```{r}
oildata <- window(oil, start=1996)
# Estimate parameters
fc <- ses(oildata, h=5)
# Accuracy of one-step-ahead training errors
round(accuracy(fc),2)
```

```{r}
# Create a training set using subset()
train <- subset(marathon, end = length(marathon) - 20)

# Compute SES and naive forecasts, save to fcses and fcnaive
fcses <- ses(train, h = 20)
fcnaive <- naive(train, h = 20)

# Calculate forecast accuracy measures
accuracy(fcses, marathon)
accuracy(fcnaive, marathon)

# Save the best forecasts as fcbest
fcbest <- fcnaive
```

### 2. Holt's Trend Smoothing

Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. The forecast function is no longer flat but trending.

lolca linear trend -slope changes 

large beta means slopes are changing quickly 
```{r}
air <- window(ausair, start=1990)
fc <- holt(air, h=5)
```

```{r}
# Produce 10 year forecasts of austa using holt()
fcholt <- holt(austa, h =10)

# Look at fitted model using summary()
summary(fcholt)

# Plot the forecasts
autoplot(fcholt)

# Check that the residuals look like white noise
checkresiduals(fcholt)
```

### 2.1 Damped Holt's Trend Methods 

Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. 
```{r}
fc <- holt(air, h=15)
fc2 <- holt(air, damped=TRUE, phi = 0.9, h=15)
autoplot(air) +
  autolayer(fc, series="Holt's method", PI=FALSE) +
  autolayer(fc2, series="Damped Holt's method", PI=FALSE) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Air passengers in Australia (millions)") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
fc <- holt(livestock, damped=TRUE)
fc[["model"]]
```

We will use time series cross-validation to compare the one-step forecast accuracy of the three methods.  
```{r}
e1 <- tsCV(livestock, ses, h=1)
e2 <- tsCV(livestock, holt, h=1)
e3 <- tsCV(livestock, holt, damped=TRUE, h=1)
```

### 3. Holt Winter

Holt (1957) and Winters (1960) extended Holt's method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations 
We apply Holt-Winters' method with both additive and multiplicative seasonality to forecast

Terms for estimating several state parameters and seasonal component.

Additive and multiplicaative versions 
```{r}
# Plot the data
autoplot(a10)

# Produce 3 year forecasts
fc <- hw(a10, seasonal = 'multiplicative', h = 36)

# Check if residuals look like white noise
checkresiduals(fc)
whitenoise <- FALSE

# Plot forecasts
autoplot(fc)
```

For subset.ts(), set the end argument to the length() of the data minus the number of days, not weeks, that you want to omit.
```{r}
# Create training data with subset()
train <- subset(hyndsight, end = length(hyndsight) - 28)

# Holt-Winters additive forecasts as fchw
fchw <- hw(train, seasonal = "additive", h = 28)

# Seasonal naive forecasts as fcsn
fcsn <- snaive(train, h = 28)

# Find better forecasts with accuracy()
accuracy(fchw, hyndsight)
accuracy(fcsn, hyndsight)

# Plot the better forecasts
autoplot(fchw)
```

```{r}
aust <- window(austourists,start=2005)
fit1 <- hw(aust,seasonal="additive")
fit2 <- hw(aust,seasonal="multiplicative")
autoplot(aust) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Visitor nights (millions)") +
  ggtitle("International visitors nights in Australia") +
  guides(colour=guide_legend(title="Forecast"))
```

damped holt winter
```{r}
hw(y, damped=TRUE, seasonal="multiplicative")
```

### 4. Other Exponential Smoothing
Exponential smoothing methods are not restricted to those we have presented so far. By considering variations in the combinations of the trend and seasonal components, nine exponential smoothing methods are possible

### 5. State Space Model and ETS Models
Each model consists of a measurement equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend, seasonal) change over time. Hence, these are referred to as state space models.

For each method there exist two models: one with additive errors and one with multiplicative errors. The point forecasts produced by the models are identical if they use the same smoothing parameter values. They will, however, generate different prediction intervals.

The models can be estimated in R using the ets() function in the forecast package. Unlike the ses(), holt() and hw() functions, the ets() function does not produce forecasts. Rather, it estimates the model parameters and returns information about the fitted model. By default it uses the AICc to select an appropriate model, although other information criteria can be selected.

The namesake function for finding errors, trend, and seasonality (ETS) provides a completely automatic way of producing forecasts for a wide range of time series.
```{r}
# Fit ETS model to austa in fitaus
fitaus <- ets(austa)

# Check residuals
checkresiduals(fitaus)

# Plot forecasts
autoplot(forecast(fitaus))

# Repeat for hyndsight data in fiths
fiths <- ets(hyndsight)
checkresiduals(fiths)
autoplot(forecast(fiths))
```

The second argument for tsCV() must return a forecast object, so you need a function to fit a model and return forecasts.
```{r}
# Function to return ETS forecasts
fets <- function(y, h) {
  forecast(ets(y), h = h)
}

# Apply tsCV() for both methods
e1 <- tsCV(cement, fets, h = 4)
e2 <- tsCV(cement, snaive, h = 4)

# Compute MSE of resulting errors (watch out for missing values)
mean(e1^2, na.rm = T)
mean(e2^2, na.rm = T)
```

```{r}
ets(y, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL,
    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,
    additive.only=FALSE, restrict=TRUE,
    allow.multiplicative.trend=FALSE)
```

y
The time series to be forecast.
model
A three-letter code indicating the model to be estimated using the ETS classification and notation. The possible inputs are N for none, A for additive, M for multiplicative, or Z for automatic selection. If any of the inputs is left as Z, then this component is selected according to the information criterion. The default value of ZZZ ensures that all components are selected using the information criterion.
damped
If damped=TRUE, then a damped trend will be used (either A or M). If damped=FALSE, then a non-damped trend will used. If damped=NULL (the default), then either a damped or a non-damped trend will be selected, depending on which model has the smallest value for the information criterion.
alpha, beta, gamma, phi
The values of the smoothing parameters can be specified using these arguments. If they are set to NULL (the default setting for each of them), the parameters are estimated.
lambda
Box-Cox transformation parameter. It will be ignored if lambda=NULL (the default value). Otherwise, the time series will be transformed before the model is estimated. When lambda is not NULL, additive.only is set to TRUE.
biasadj
If TRUE and lambda is not NULL, then the back-transformed fitted values and forecasts will be bias-adjusted.
additive.only
Only models with additive components will be considered if additive.only=TRUE. Otherwise, all models will be considered.
restrict
If restrict=TRUE (the default), the models that cause numerical difficulties are not considered in model selection.
allow.multiplicative.trend
Multiplicative trend models are also available, but not covered in this book. Set this argument to TRUE to allow these models to be considered.
Working with ets objects
The ets() function will return an object of class ets. There are many R functions designed to make working with ets objects easy. A few of them are described below.

coef()
returns all fitted parameters.
accuracy()
returns accuracy measures computed on the training data.
summary()
prints some summary information about the fitted model.
autoplot() and plot()
produce time plots of the components.
residuals()
returns residuals from the estimated model.
fitted()
returns one-step forecasts for the training data.
simulate()
will simulate future sample paths from the fitted model.
forecast()
computes point forecasts and prediction intervals, as described in the next section.

```{r}
aust <- window(austourists, start=2005)
fit <- ets(aust)
summary(fit)
autoplot(fit)
```

#### Forecasting with ETS

A big advantage of the models is that prediction intervals can also be generated - something that cannot be done using the methods. The prediction intervals will differ between models with additive and multiplicative methods.
```{r}
forecast(object, h=ifelse(object$m>1, 2*object$m, 10),
level=c(80,95), fan=FALSE, simulate=FALSE, bootstrap=FALSE,
npaths=5000, PI=TRUE, lambda=object$lambda, biasadj=NULL, ...)
```

object
The object returned by the ets() function.
h
The forecast horizon - the number of periods to be forecast.
level
The confidence level for the prediction intervals.
fan
If fan=TRUE, level=seq(50,99,by=1). This is suitable for fan plots.
simulate
If simulate=TRUE, prediction intervals are produced by simulation rather than using algebraic formulas. Simulation will also be used (even if simulate=FALSE) where there are no algebraic formulas available for the particular model.
bootstrap
If bootstrap=TRUE and simulate=TRUE, then the simulated prediction intervals use re-sampled errors rather than normally distributed errors.
npaths
The number of sample paths used in computing simulated prediction intervals.
PI
If PI=TRUE, then prediction intervals are produced; otherwise only point forecasts are calculated.
lambda
The Box-Cox transformation parameter. This is ignored if lambda=NULL. Otherwise, the forecasts are back-transformed via an inverse Box-Cox transformation.
biasadj
If lambda is not NULL, the back-transformed forecasts (and prediction intervals) are bias-adjusted.

# D. ARIMA

ARMA models: AR - autoregresive, MA - autocorrelated errors

ARMA model can model any stationary series (Theorem by Hermann Wold - Wold Decomposition: any stationary time series may be represented as linear combination of white noise and all arma models can be represented by moving average by invertibility)

Simulate stationary series in R:
1. $X_{t} = W_{t} + 0.9W_{t-1}$
```{r}
# model = list with order of the model as c(p, d, q)
x <- arima.sim(list(order = c(0, 0, 1), ma = 0.9), 
          n = 100)
plot(x)
```

2. $X_{t} = -0.9X_{t-2} + W_{t}$
```{r}
x <- arima.sim(list(orer = c(2, 0, 0), ar = c(0, -0.9)), n = 100)
plot(x)
```

```{r}
# Generate and plot white noise
WN <- arima.sim(model = list(order = c(0, 0, 0)), n = 200)
plot(WN)

# Generate and plot an MA(1) with parameter .9 
MA <- arima.sim(model = list(order = c(0, 0, 1), ma = 0.9), n = 200)
plot(MA)


# Generate and plot an AR(2) with parameters 1.5 and -.75
AR <- arima.sim(model = list(orer = c(2, 0, 0), ar = c(1.5, -0.75)), n = 200)
plot(AR)
```

# How to Identify AR(p) and MA(q) from data

By looking at data, you cannot.
You need to look at ACF and PACF 
For AR(p), ACF tails off and PACF cutss off lag p
For MA(q), ACF cuts off lagq and PACF tails off
For ARMA(p, q), both ACF and PACF tail off

It is not possible for both ACF and PACF to cut off

We should start from ARMA(1,1) and see from there
```{r}
# Generate 100 observations from the AR(1) model
x <- arima.sim(model = list(order = c(1, 0, 0), ar = .9), n = 100) 

# Plot the generated data 
plot(x)

# Plot the sample P/ACF pair
acf2(x)

# Fit an AR(1) to the data and examine the t-table
sarima(x, p = 1, d = 0, q = 0)

```

Stationary series can have autocorrelated structure (=ARMA(1,1))
The best approach to fitting ARMA is to start with a low order model, and then try to add a parameter at a time to see if the results change.

ARMA models selection criteria: AIC and BIC
when AIC/BIC values are similar betwen models choose a simpler model (smaller order of p or q) The idea is based on the parsimony principle, which is basic to all science and tells you to choose the simplest scientific explanation that fits the evidence.
But we still want residuals to be white Gaussian noise. We can check by sarima() function of astsa. Check whiteness of residuals by ACF plot (95% of residuals should be within the band)

Q-statistic p-values should be above the blue line. Else, there is some correlation left in the residuals and we should try to fit another model

Bad residuals show patterns like periodicity, large ACF values. Q statistics below line

sarima() run includes a residual analysis graphic. Specifically, the output shows (1) the standardized residuals, (2) the sample ACF of the residuals, (3) a normal Q-Q plot, and (4) the p-values corresponding to the Box-Ljung-Pierce Q-statistic.

In each run, check the four residual plots as follows:

The standardized residuals should behave as a white noise sequence with mean zero and variance one. Examine the residual plot for departures from this behavior.
The sample ACF of the residuals should look like that of white noise. Examine the ACF for departures from this behavior.
Normality is an essential assumption when fitting ARMA models. Examine the Q-Q plot for departures from normality and to identify outliers.
Use the Q-statistic plot to help test for departures from whiteness of the residuals.
```{r}
# Fit an MA(1) to dl_varve. Examine the residuals  
sarima(dl_varve,0,0,1)

# Fit an ARMA(1,1) to dl_varve. Examine the residuals
sarima(dl_varve, 1,0,1)
```

## ARIMA 

A time series exhibits ARIAM behavior if the differenced data has ARMA behavior
```{r}
# Plot sample P/ACF of differenced data and determine model
acf2(diff(x))


# Estimate parameters and examine output
sarima(x, 2, 1, 0)
```

The ACF and the PACF are both tailing off, implying an ARIMA(1,1,1) model.
The ACF cuts off at lag 2, and the PACF is tailing off, implying an ARIMA(0,1,2) model.
The ACF is tailing off and the PACF cuts off at lag 3, implying an ARIMA(3,1,0) model. Although this model fits reasonably well, it is the worst of the three (you can check it) because it uses too many parameters for such small autocorrelations.

### Moving Average Submodel

A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values.

2. plot of errors vs actual values
3. compare models by AIC/BIC
4. check mean, autocorrelation, normality of residuals. confirm if they are white noise.
5. for multiple forecasts, plot the mean, p-value of autocorrelation test, and p-value of normality
6. acf plot, pacf plot
7. Durbin-Watson test, Box-Ljung test

The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased. Any forecasting method that does not satisfy these properties can be improved. However, that does not mean that forecasting methods that satisfy these properties cannot be improved. It is possible to have several different forecasting methods for the same data set, all of which satisfy these properties. Checking these properties is important in order to see whether a method is using all of the available information, but it is not a good way to select a forecasting method. If either of these properties is not satisfied, then the forecasting method can be modified to give better forecasts. Adjusting for bias is easy: if the residuals have mean  , then simply add to all forecasts and the bias problem is solved. Fixing the correlation problem is harder,

 it is useful (but not necessary) for the residuals to also have the following two properties.

The residuals have constant variance.
The residuals are normally distributed.
These two properties make the calculation of prediction intervals easier 

### Checking Overfitting

Diagnostics - simulated overfitting
One way to check an analysis is to overfit the model by adding an extra parameter to see if it makes a difference in the results. If adding parameters changes the results drastically, then you should rethink your model. If, however, the results do not change by much, you can be confident that your fit is correct.
```{r}
# Plot sample P/ACF pair of the differenced data
acf2(diff(x))

# Fit the first model, compare parameters, check diagnostics
sarima(x, 0, 1, 1)

# Fit the second model and compare fit
sarima(x,0,1,2)
```

As you can see from the t-table, the second MA parameter is not significantly different from zero and the first MA parameter is approximately the same in each run. Also, the AIC and BIC both increase when the parameter is added. In addition, the residual analysis of your ARIMA(0,1,1) model is fine. All of these facts together indicate that you have a successful model fit.

### Forecasting

The basic syntax for forecasting is sarima.for(data, n.ahead, p, d, q) where n.ahead is a positive integer that specifies the forecast horizon. The predicted values and their standard errors are printed, the data are plotted in black, and the forecasts are in red along with 2 mean square prediction error bounds as blue dashed lines.
```{r}
# Plot P/ACF pair of differenced data 
acf2(diff(x))

# Fit model - check t-table and diagnostics
sarima(x,1,1,0)

# Forecast the data 20 time periods ahead
sarima.for(x, n.ahead = 20, p = 1, d = 1, q = 0) 
lines(y)  
```

## Seasonal Adjustment

### Pure Seasonal

 a pure seasonal ARMA time series is correlated at the seasonal lags only. Consequently, the ACF and PACF behave as the nonseasonal counterparts, but at the seasonal lags, 1S, 2S, ..., where S is the seasonal period (S = 12 for monthly data). As with other models, you can fit seasonal models in R using the sarima() command in the astsa package.
```{r}
# Plot sample P/ACF to lag 60 and compare to the true values
acf2(x, max.lag = 60)

# Fit the seasonal model to x
sarima(x, p = 0, d = 0, q = 0, P = 1, D = 0, Q = 1, S = 12)

```

### Mixed Seasonal

Most seasonal time series have mixed dependence, meaning only some of the variation is explained by seasonal trends.
Recall that the full seasonal model is denoted by SARIMA(p,d,q)x(P,D,Q)S where capital letters denote the seasonal orders. as opposed to the pure seasonal model, there are correlations at the nonseasonal lags as well as the seasonal lags.

SAR(1): Value this month is related to last year's value; strong, pronounced spikes for seasonal ACF values
AR(1): value this month is related to the last month's value: fading acf values in the nearest months

Seasonal AR: ACF tails off and PACF cuts off at asesonal lag 1
Non-seasonal: lags ACF cutoff at lag 1 and tail off for PACF -> MA(1)

Extracted aseasonal component: barely changing from year to year is called seasonal persistence. Easir to model with it removed by seasonal differencing
```{r}
# Plot differenced chicken
plot(diff(chicken))
```

Note that the trend is removed and note the seasonal behavior
```{r}
# Plot P/ACF pair of differenced data to lag 60
acf2(diff(chicken), max.lag = 60)
```

Notice that an AR(2) seems appropriate but there is a small but significant seasonal component remaining in the detrended data.

Fit an ARIMA(2,1,0) to the chicken data to see that there is correlation remaining in the residuals.
```{r}
# Fit ARIMA(2,1,0) to chicken - not so good
sarima(chicken, 2, 1, 0)
```

```{r}
# Fit SARIMA(2,1,0,1,0,0,12) to chicken - that works
sarima(chicken, 2,1,0,1,0,0,12)
```

Once the model is well fit, we understand the dynamics of time series. we simply continune the dynamics over to the future.
```{r}
# Forecast the data 3 years into the future
sarima.for(unemp, 2,1,0,0,1,1,12,n.ahead=36)
```

### auto.arima()

ARIMA models provide another approach to time series forecasting. Exponential smoothing and ARIMA models are the two most widely-used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

AR models: multiple regression with algged observations as predictors

MA models: multiple regression with lagged errors as predictors

ARMA models: multiple regression with lagged observations and errors as predictors

ARIMA models: combine ARMA model with lots of differencing

Drift is a $c$ term in differencing

You can only copare models of the same class. or models of different amounts of differencing

Autom.airma() follows hydman-Kandakar algorithm

select number of differences d via unit root tests
Select p and q by minimizing AIC
Estimate parameters using maximum likelihood estimation
use stepwise search to traverse model space to save time (so we dont necessariyly getm odle with the best AIC )

auto.arima() function will select an appropriate autoregressive integrated moving average (ARIMA) model given a time series, just like the ets() function does for ETS models. The summary() function can provide some additional insights

check that the residuals of the resulting model look like white noise
```{r}
# Fit an automatic ARIMA model to the austa series
fit <- auto.arima(austa)

# Check that the residuals look like white noise
checkresiduals(fit)
residualsok <- TRUE

# Summarize the model
summary(fit)

# Find the AICc value and the number of differences used
AICc <- -14.46
d <- 1

# Plot forecasts of fit
fit %>% forecast(h = 10) %>% autoplot()
```

A model with $d=0$  and no constant will have forecasts that converge to  0 .
A model with  $d=0$  and a constant will have forecasts that converge to the mean of the data.
A model with  $d=1$  and no constant will have forecasts that converge to a non-zero value close to the last observation.
A model with  $d=1$  and a constant will have forecasts that converge to a linear function with slope based on the whole series.
A model with  $d=2$  and no constant will have forecasts that converge to a linear function with slope based on the last few observations.
```{r}
# Plot forecasts from an ARIMA(0,1,1) model with no drift
austa %>% Arima(order = c(0, 1, 1), include.constant = FALSE) %>% forecast() %>% autoplot()

# Plot forecasts from an ARIMA(2,1,3) model with drift
austa %>% Arima(order = c(2, 1, 3), include.constant = TRUE) %>% forecast() %>% autoplot()

# Plot forecasts from an ARIMA(0,0,1) model with a constant
austa %>% Arima(order = c(0, 0, 1), include.constant = TRUE) %>% forecast() %>% autoplot()

# Plot forecasts from an ARIMA(0,2,1) model with no constant
austa %>% Arima(order = c(0, 2, 1), include.constant = FALSE) %>% forecast() %>% autoplot()
```

Without including drift or constant term, you can create trend component using 

#### Seasonal Auto.arima

the auto.arima() function also works with seasonal data. Note that setting lambda = 0 in the auto.arima() function - applying a log transformation - means that the model will be fitted to the transformed data, and that the forecasts will be back-transformed onto the original scale.
```{r}
# Check that the logged h02 data have stable variance
h02 %>% log() %>% autoplot()

# Fit a seasonal ARIMA model to h02 with lambda = 0
fit <- auto.arima(h02, lambda = 0)

# Summarize the fitted model
summary(fit)

# Record the amount of lag-1 differencing and seasonal differencing used
d <- 1
D <- 1

# Plot 2-year forecasts
fit %>% forecast(h=24) %>% autoplot()
```

The auto.arima() function needs to estimate a lot of different models, and various short-cuts are used to try to make the function as fast as possible. This can cause a model to be returned which does not actually have the smallest AICc value. To make auto.arima() work harder to find a good model, add the optional argument stepwise = FALSE to look at a much larger collection of models.
```{r}
# Don't use a stepwise search
fit2 <- auto.arima(euretail, stepwise=FALSE)
summary(fit2)
# Compute 2-year forecasts from better model
fit2 %>% forecast(h=8) %>% autoplot()
```

## Comparing ETS and ARRIMA

Comparing auto.arima() and ets() on non-seasonal data
The AICc statistic is useful for selecting between models in the same class. For example, you can use it to select an ETS model or to select an ARIMA model. However, you cannot use it to compare ETS and ARIMA models because they are in different model classes.

Instead, you can use time series cross-validation to compare an ARIMA model and an ETS model on the austa data. Because tsCV() requires functions that return forecast objects, you will set up some simple functions that fit the models and return the forecasts. The arguments of tsCV() are a time series, forecast function, and forecast horizon h
```{r}
# Set up forecast functions for ETS and ARIMA models
fets <- function(x, h) {
  forecast(ets(x), h = h)
}
farima <- function(x, h) {
  forecast(auto.arima(x), h = h)
}

# Compute CV errors for ETS on austa as e1
e1 <- tsCV(austa, fets, h=1)

# Compute CV errors for ARIMA on austa as e2
e2 <- tsCV(austa, farima, h=1)

# Find cross-validated MSE of each model class
mean(e1^2, na.rm=T)
mean(e2^2, na.rm=T)

# Plot 10-year forecasts using the best model class
austa %>% farima(h=10) %>% autoplot()
```

Comparing auto.arima() and ets() on seasonal data
What happens when you want to create training and test sets for data that is more frequent than yearly? If needed, you can use a vector in form c(year, period) for the start and/or end keywords in the window() function. You must also ensure that you're using the appropriate values of h in forecasting functions. Recall that h should be equal to the length of the data that makes up your test set.

For example, if your data spans 15 years, your training set consists of the first 10 years, and you intend to forecast the last 5 years of data, you would use h = 12 * 5 not h = 5 because your test set would include 60 monthly observations. If instead your training set consists of the first 9.5 years and you want forecast the last 5.5 years, you would use h = 66 to account for the extra 6 months.
```{r}
# Use 20 years of the qcement data beginning in 1988
train <- window(qcement, start = 1988, end = c(2007,4))
test <- window(qcement, start = 2008)

# Fit an ARIMA and an ETS model to the training data
fit1 <- auto.arima(train)
fit2 <- ets(train)

# Check that both models have white noise residuals
checkresiduals(fit1)
checkresiduals(fit2)

# Produce forecasts for each model
fc1 <- forecast(fit1, h = 25)
fc2 <- forecast(fit2, h = 25)

# Use accuracy() to find better model based on RMSE
accuracy(fc1, qcement)
accuracy(fc2, qcement)
bettermodel <- fit2
```

```{r}
fit <- auto.arima(uschange[,"Consumption"], seasonal=FALSE)
(fit2 <- Arima(uschange[,"Consumption"], order=c(3,0,0)))
(fit3 <- auto.arima(uschange[,"Consumption"], seasonal=FALSE,
  stepwise=FALSE, approximation=FALSE))
fit %>% forecast(h=10) %>% autoplot(include=80)
checkresiduals(fit)
```

it is sometimes possible to use the ACF plot, and the closely related PACF plot, to determine appropriate values for  p and q

To overcome this problem, we can use partial autocorrelations. These measure the relationship between So the first partial autocorrelation is identical to the first autocorrelation, because there is nothing between them to remove. Each partial autocorrelation can be estimated as the last coefficient in an autoregressive model. 
partial autocorrelation coefficient, is equal to the estimate
```{r}
ggAcf(uschange[,"Consumption"])
ggPacf(uschange[,"Consumption"])
```

The auto.arima() function in R uses a variation of the Hyndman-Khandakar algorithm combines unit root tests, minimisation of the AICc and MLE to obtain an ARIMA model

Hyndman-Khandakar algorithm for automatic ARIMA modelling
1. The number of differences $0 <= d <= 2$ is determined using repeated KPSS tests.

2. The values of  p and  qare then chosen by minimising the AICc after differencing the data  d times. Rather than considering every possible combination of   p and  q, the algorithm uses a stepwise search to traverse the model space.

The best model (with the smallest AICc value) fitted in step (a) is set to be the "current model".
Variations on the current model are considered:
vary  
p
  and/or  
q
  from the current model by  
1
 ;
include/exclude  
c
  from the current model.
The best model considered so far (either the current model or one of these variations) becomes the new current model.
Repeat Step 2(c) until no lower AICc can be found.

If you want to choose the model yourself, use the Arima() function in R. There is another function arima() in R which also fits an ARIMA model. However, it does not allow for the constant  
c
  unless  
d
=
0
 , and it does not return everything required for other functions in the forecast package to work. Finally, it does not allow the estimated model to be applied to new data (which is useful for checking forecast accuracy). Consequently, it is recommended that Arima() be used instead.

# auto.arima seasonal

```{r}
elecequip %>% stl(s.window='periodic') %>% seasadj() -> eeadj
autoplot(eeadj)
eeadj %>% diff() %>% ggtsdisplay(main="")
```

## Seasonality

```{r}
gasoline %>% stlf() %>% autoplot()
```

```{r}
bestfit <- list(aicc=Inf)
for(K in seq(25)) {
  fit <- auto.arima(gasoline, xreg=fourier(gasoline, K=K),
    seasonal=FALSE)
  if(fit[["aicc"]] < bestfit[["aicc"]]) {
    bestfit <- fit
    bestK <- K
  }
}
fc <- forecast(bestfit,
  xreg=fourier(gasoline, K=bestK, h=104))
autoplot(fc)
```

## Constants in ARIAM
 the inclusion of a constant in a non-stationary ARIMA model is equivalent to inducing a polynomial trend of order  
d in the forecast function.
  
### Unit Roots within a Circle
```{r}
autoplot(fit)
```

# ETS vs ARIMA
ARIMA models provide another approach to time series forecasting. Exponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

It is a commonly held myth that ARIMA models are more general than exponential smoothing. While linear exponential smoothing models are all special cases of ARIMA models, the non-linear exponential smoothing models have no equivalent ARIMA counterparts. On the other hand, there are also many ARIMA models that have no exponential smoothing counterparts. In particular, all ETS models are non-stationary, while some ARIMA models are stationary.

The ETS models with seasonality or non-damped trend or both have two unit roots (i.e., they need two levels of differencing to make them stationary). All other ETS models have one unit root (they need one level of differencing to make them stationary).

```{r}
fets <- function(x, h) {
  forecast(ets(x), h = h)
}
farima <- function(x, h) {
  forecast(auto.arima(x), h=h)
}
# Compute CV errors for ETS as e1
e1 <- tsCV(air, fets, h=1)
# Compute CV errors for ARIMA as e2
e2 <- tsCV(air, farima, h=1)
# Find MSE of each model class
mean(e1^2, na.rm=TRUE)
```

```{r}
air %>% ets() %>% forecast() %>% autoplot()
```

## Create Train and Test Sets
```{r}
train <- window(oil, end=2003)
test <- window(oil, start=2004)


# Consider the qcement data beginning in 1988
cement <- window(qcement, start=1988)
# Use 20 years of the data as the training set
train <- window(cement, end=c(2007,4))
(fit.arima <- auto.arima(train))
checkresiduals(fit.arima)

```

# Forecast On test set

```{r}
# Generate forecasts and compare accuracy over the test set
a1 <- fit.arima %>% forecast(h = 4*(2013-2007)+1) %>%
  accuracy(qcement)
a1[,c("RMSE","MAE","MAPE","MASE")]

a2 <- fit.ets %>% forecast(h = 4*(2013-2007)+1) %>%
  accuracy(qcement)
a2[,c("RMSE","MAE","MAPE","MASE")]
```

Notice that the ARIMA model fits the training data slightly better than the ETS model, but that the ETS model provides more accurate forecasts on the test set. A good fit to training data is never an indication that the model will forecast well.

```{r}
# Generate forecasts from an ETS model
cement %>% ets() %>% forecast(h=12) %>% autoplot()
```

## Dynamic Regression

The time series models in the previous chapters work well for many time series, but they are often not good for weekly or hourly data, and they do not allow for the inclusion of other information such as the effects of holidays, competitor activity, changes in the law, etc. In this chapter, you will look at some methods that handle more complicated seasonality, and you consider how to extend ARIMA models in order to allow other information to be included in the them.

Error term is an ARIAM process
$y_{t} = \beta_{0} + \beta_{1} x_{1,t} + ... \beta_{r} x_{r,t} + e_{t}$ whereas in OLS $e_{t}$ is a white noise 

In dynamic regression model, the regression part takes account of the predictor variable, while the ARIMA model takes care of the short term time series dynamics. The Ljung box test needs to have a p-value above 0.05 which looks like white noise 

The auto.arima() function will fit a dynamic regression model with ARIMA errors. The only change to how you used it previously is that you will now use the xreg argument containing a matrix of regression variables
```{r}
# The dynamic regression allows you to include other outside information into your forecast.
# Time plot of both variables
autoplot(advert, facets=TRUE)

# Fit ARIMA model
fit <- auto.arima(advert[, 'sales'], xreg = advert[, 'advert'], stationary = TRUE)

# Check model. Increase in sales for each unit increase in advertising
salesincrease <- coefficients(fit)[3]

# Forecast fit as fc
fc <- forecast(fit, xreg = rep(10,6))

# Plot fc with x and y labels
autoplot(fc) + xlab('Month') + ylab('Sales')

# Multiple Exogenous variables
# Time plots of demand and temperatures
autoplot(elec[, c('Demand', 'Temperature')], facets = TRUE)

# Matrix of regressors
xreg <- cbind(MaxTemp = elec[, "Temperature"], 
              MaxTempSq = (elec[,'Temperature'])^2, 
              Workday = elec[,'Workday'])

# Fit model
fit <- auto.arima(elec[,'Demand'], xreg = xreg)

# Forecast fit one day ahead
forecast(fit, xreg = cbind(20, 400, 1))
```

### Dynamic Harmonic Regression

Uses Fourerirterms to handle seasonality .Fouerir theorem: any periodic function can be approximated by combination ofsine and cosine 

One weakness: Fourerir model doesnt allow seasonalit to change over time while ARIMa does 


The R function Arima() will fit a regression model with ARIMA errors if the argument xreg is used. The order argument specifies the order of the ARIMA error model. If differencing is specified, then the differencing is applied to all variables in the regression model before the model is estimated. 
To include a constant in the differenced model, specify include.drift=TRUE.
```{r}
fit <- Arima(y, xreg=x, order=c(1,1,0))
```

The auto.arima() function will also handle regression terms via the xreg argument. The user must specify the predictor variables to include, but auto.arima() will select the best ARIMA model for the errors. If differencing is required, then all variables are differenced during the estimation process, although the final model will be expressed in terms of the original variables.

The AICc is calculated for the final model, and this value can be used to determine the best predictors. That is, the procedure should be repeated for all subsets of predictors to be considered, and the model with the lowest AICc value selected.

```{r}
fcast <- forecast(fit, xreg=rep(mean(uschange[,2]),8))
autoplot(fcast) + xlab("Year") +
  ylab("Percentage change")
```


```{r}
xreg <- cbind(MaxTemp = elecdaily[, "Temperature"],
              MaxTempSq = elecdaily[, "Temperature"]^2,
              Workday = elecdaily[, "WorkDay"])
fit <- auto.arima(elecdaily[, "Demand"], xreg = xreg)
checkresiduals(fit)
fcast <- forecast(fit,
  xreg = cbind(MaxTemp=rep(26,14), MaxTempSq=rep(26^2,14),
    Workday=c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))
autoplot(fcast) + ylab("Electricity demand (GW)")

```

# Stochatic vs Deterministic Trend

```{r}
# Deterinistic
autoplot(austa) + xlab("Year") +
  ylab("millions of people") +
  ggtitle("Total annual international visitors to Australia")
trend <- seq_along(austa)
(fit1 <- auto.arima(austa, d=0, xreg=trend))
```

 In particular, stochastic trends have much wider prediction intervals because the errors are non-stationary.
```{r}
# Stochastic
(fit2 <- auto.arima(austa, d=1))
fc1 <- forecast(fit1,
  xreg = length(austa) + 1:10)
fc2 <- forecast(fit2, h=10)
autoplot(austa) +
  autolayer(fc2, series="Stochastic trend") +
  autolayer(fc1, series="Deterministic trend") +
  ggtitle("Forecasts from trend models") +
  xlab("Year") + ylab("Visitors to Australia (millions)") +
  guides(colour=guide_legend(title="Forecast"))
```
There is an implicit assumption with deterministic trends that the slope of the trend is not going to change over time. On the other hand, stochastic trends can change, and the estimated growth is only assumed to be the average growth over the historical period, not necessarily the rate of growth that will be observed into the future. Consequently, it is safer to forecast with stochastic trends, especially for longer forecast horizons, as the prediction intervals allow for greater uncertainty in future growth.

# Dynamic Regression with Fourier Series

When there are long seasonal periods, a dynamic regression with Fourier terms is often better than other models we have considered in this book.

We select K - start with K = 1, slowly increase it until the AIC value stops decreasing. Using K is to capture seasonality.When AIC's decrease and it increasses, you can see there is less signal in the residuals. 

With weekly data, it is difficult to handle seasonality using ETS or ARIMA models as the seasonal length is too large (approximately 52). Instead, you can use harmonic regression which uses sines and cosines to model the seasonality.

The fourier() function makes it easy to generate the required harmonics. The higher the order (K), the more "wiggly" the seasonal pattern is allowed to be. With K=1, it is a simple sine curve. You can select the value of K by minimizing the AICc value. fourier() takes in a required time series, required number of Fourier terms to generate, and optional number of rows it needs to forecast

Other predictors can be added as well. You can choose K to minimize the AIC, and K can not be more than m/2 (K cant be more than half the seasonal period). This is particularly useful for weekly data, daily data, and sub-daily data.

The advantage of Fourerir series is that they can handle seasonality when the seasonal period m is very large 
```{r}
# Set up harmonic regressors of order 13
harmonics <- fourier(gasoline, K = 13)

# Fit regression model with ARIMA errors
fit <- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE)

# Forecasts next 3 years
newharmonics <- fourier(gasoline, K = 13, h = 156)
fc <- forecast(fit, xreg = newharmonics)

# Plot forecasts fc
autoplot(fc)
```

Harmonic regressions are also useful when time series have multiple seasonal patterns.

auto.arima() would take a long time to fit a long time series such as this one, so instead you will fit a standard regression model with Fourier terms using the tslm() function. This is very similar to lm() but is designed to handle time series. With multiple seasonality, you need to specify the order K for each of the seasonal periods.
```{r}
# Fit a harmonic regression using order 10 for each type of seasonality
fit <- tslm(taylor ~ fourier(taylor, K = c(10, 10)))

# Forecast 20 working days ahead. The data are half-hourly 
fc <- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10,10), h = 960)))

# Plot the forecasts
autoplot(fc)

# Check the residuals of fit
checkresiduals(fit)
```

```{r}
# Plot the calls data
autoplot(calls)

# Set up the xreg matrix using order 10 for daily seasonality and 0 for weekly seasonality. The weekly seasonality is relatively weak, so here you will just model daily seasonality. 
xreg <- fourier(calls, K = c(10,0)) # Note that if you incorrectly specify your vector, your session may expire!

# Fit a dynamic regression model with seasonal = FALSE and stationary = TRUE
fit <- auto.arima(calls, xreg = xreg, seasonal = FALSE, stationary = TRUE)

# Check the residuals
checkresiduals(fit)

# Plot forecasts for 10 working days ahead; data are rows of every 5 minutes; There are 169 5-minute periods in a working day
fc <- forecast(fit, xreg =  fourier(calls, c(10, 0), h = 1690))
autoplot(fc)
```

The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant. This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series. The effect of the remaining correlations on the forecasts will be negligible
```{r}
cafe04 <- window(auscafe, start=2004)
plots <- list()
for (i in seq(6)) {
  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i),
    seasonal = FALSE, lambda = 0)
  plots[[i]] <- autoplot(forecast(fit,
      xreg=fourier(cafe04, K=i, h=24))) +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
    ylab("") + ylim(1.5,4.7)
}
gridExtra::grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

Sometimes, the impact of a predictor which is included in a regression model will not be simple and immediate. For example, an advertising campaign may impact sales for some time beyond the end of the campaign, and sales in one month will depend on the advertising expenditure in each of the past few months. Similarly, a change in a company's safety policy may reduce accidents immediately, but have a diminishing effect over time as employees take less care when they become familiar with the new working conditions.

In these situations, we need to allow for lagged effects of the predictor. Suppose that we have only one predictor in our model. 
```{r}
# Lagged predictors. Test 0, 1, 2 or 3 lags.
Advert <- cbind(
    AdLag0 = insurance[,"TV.advert"],
    AdLag1 = stats::lag(insurance[,"TV.advert"],-1),
    AdLag2 = stats::lag(insurance[,"TV.advert"],-2),
    AdLag3 = stats::lag(insurance[,"TV.advert"],-3)) %>%
  head(NROW(insurance))

# Restrict data so models use same fitting period
fit1 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1],
  stationary=TRUE)
fit2 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:2],
  stationary=TRUE)
fit3 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:3],
  stationary=TRUE)
fit4 <- auto.arima(insurance[4:40,1], xreg=Advert[4:40,1:4],
  stationary=TRUE)
c(fit1[["aicc"]],fit2[["aicc"]],fit3[["aicc"]],fit4[["aicc"]])
```

```{r}
(fit <- auto.arima(insurance[,1], xreg=Advert[,1:2],
  stationary=TRUE))
fc8 <- forecast(fit, h=20,
  xreg=cbind(AdLag0 = rep(8,20),
             AdLag1 = c(Advert[40,1], rep(8,19))))
autoplot(fc8) + ylab("Quotes") +
  ggtitle("Forecast quotes with future advertising set to 8")
```

# Dynamic Harmonic Regression with Multiple Seasonal Periods

```{r}
fit <- auto.arima(calls, seasonal=FALSE, lambda=0,
         xreg=fourier(calls, K=c(10,10)))
fit %>%
  forecast(xreg=fourier(calls, K=c(10,10), h=2*169)) %>%
  autoplot(include=5*169) +
    ylab("Call volume") + xlab("Weeks")

```

# TBATS Model

Combines many components of the models we have covered so far. IT contains trigonometric terms for seasonality (similar to Fourerir but change over time), Box-Cox transformations for heterogeneity, ARMA errors for short-term dynamics, trend (possibly damped), Seasonal (including multiple and non-integer periods). autoplot() function shows the parameters used in the title: 1. box-cox transformation parameter 2. ARMA parameters 3. damping parameter (- means no damping) 4. Fourier terms 
a TBATS model is a special kind of time series model. It can be very slow to estimate, especially with multiple seasonal time series
```{r}
# Plot the gas data
autoplot(gas)

# Fit a TBATS model to the gas data
fit <- tbats(gas)

# Forecast the series for the next 5 years
fc <- forecast(fit, h = 60)

# Plot the forecasts
autoplot(fc)

# Record the Box-Cox parameter and the order of the Fourier terms
lambda <- 0.082
K <- 5 # the second parameter <12,5>
```

```{r}
calls %>%
  subset(start=length(calls)-2000) %>%
  tbats() -> fit2
fc2 <- forecast(fit2, h=2*169)
autoplot(fc2, include=5*169) +
  ylab("Call volume") + xlab("Weeks")
```

# Hierarchical Time Series

Time series can often be naturally disaggregated by various attributes of interest. For example, the total number of bicycles sold by a cycling manufacturer can be disaggregated by product type such as road bikes, mountain bikes, children's bikes and hybrids. Each of these can be disaggregated into finer categories. For example hybrid bikes can be divided into city, commuting, comfort, and trekking bikes; and so on. These categories are nested within the larger group categories, and so the collection of time series follow a hierarchical aggregation structure. Therefore we refer to these as "hierarchical time series"
```{r}
library(hts)
tourism.hts <- hts(visnights, characters = c(3, 5))
tourism.hts %>% aggts(levels=0:1) %>%
  autoplot(facet=TRUE) +
  xlab("Year") + ylab("millions") + ggtitle("Visitor nights")
```

The aggts() function extracts time series from an hts object for any level of aggregation.
```{r}
library(tidyverse)
cols <- sample(scales::hue_pal(h=c(15,375),
  c=100,l=65,h.start=0,direction = 1)(NCOL(visnights)))
as_tibble(visnights) %>%
  gather(Zone) %>%
  mutate(Date = rep(time(visnights), NCOL(visnights)),
         State = str_sub(Zone,1,3)) %>%
  ggplot(aes(x=Date, y=value, group=Zone, colour=Zone)) +
    geom_line() +
    facet_grid(State~., scales="free_y") +
    xlab("Year") + ylab("millions") +
    ggtitle("Visitor nights by Zone") +
    scale_colour_manual(values = cols)
```
 
```{r}
forecast(prison.gts, method="bu", fmethod="arima")
```

```{r}
prisonfc <- forecast(prison.gts)
```

```{r}
fcsts <- aggts(prisonfc, levels=0:3)
```

```{r}
groups <- aggts(prison.gts, levels=0:3)
autoplot(fcsts) + autolayer(groups)
prisonfc <- ts(rbind(groups, fcsts),
  start=start(groups), frequency=4)
p1 <- autoplot(prisonfc[,"Total"]) +
  ggtitle("Australian prison population") +
  xlab("Year") + ylab("Total number of prisoners ('000)") +
  geom_vline(xintercept=2017)
cols <- sample(scales::hue_pal(h=c(15,375),
          c=100,l=65,h.start=0,direction = 1)(NCOL(groups)))
p2 <- as_tibble(prisonfc[,-1]) %>%
  gather(Series) %>%
  mutate(Date = rep(time(prisonfc), NCOL(prisonfc)-1),
         Group = str_extract(Series, "([A-Za-z ]*)")) %>%
  ggplot(aes(x=Date, y=value, group=Series, colour=Series)) +
    geom_line() +
    xlab("Year") + ylab("Number of prisoners ('000)") +
    scale_colour_manual(values = cols) +
    facet_grid(. ~ Group, scales="free_y") +
    scale_x_continuous(breaks=seq(2006,2018,by=2)) +
    theme(axis.text.x = element_text(angle=90, hjust=1)) +
    geom_vline(xintercept=2017)
gridExtra::grid.arrange(p1, p2, ncol=1)
```

# Vector Autoregressions

```{r}
VARselect(uschange[,1:2], lag.max=8,
  type="const")[["selection"]]

var1 <- VAR(uschange[,1:2], p=1, type="const")
serial.test(var1, lags.pt=10, type="PT.asymptotic")
var2 <- VAR(uschange[,1:2], p=2, type="const")
serial.test(var2, lags.pt=10, type="PT.asymptotic")

var3 <- VAR(uschange[,1:2], p=3, type="const")
serial.test(var3, lags.pt=10, type="PT.asymptotic")

forecast(var3) %>%
  autoplot() + xlab("Year")
```

# Neural Network Autoregressions

```{r}
fit <- nnetar(sunspotarea, lambda=0)
autoplot(forecast(fit,h=30))
fcast <- forecast(fit, PI=TRUE, h=30)
autoplot(fcast)
```

# Ensemble of Forecasts

```{r}
etsfc <- debitcards %>% ets() %>% forecast(h=36)
baggedfc <- debitcards %>% baggedETS() %>% forecast(h=36)
autoplot(debitcards) +
  autolayer(baggedfc, series="BaggedETS", PI=FALSE) +
  autolayer(etsfc, series="ETS", PI=FALSE) +
  guides(colour=guide_legend(title="Forecasts"))
```

# Combination of Forecasts

```{r}
train <- window(auscafe, end=c(2012,9))
h <- length(auscafe) - length(train)
ETS <- forecast(ets(train), h=h)
ARIMA <- forecast(auto.arima(train, lambda=0, biasadj=TRUE),
  h=h)
STL <- stlf(train, lambda=0, h=h, biasadj=TRUE)
NNAR <- forecast(nnetar(train), h=h)
TBATS <- forecast(tbats(train, biasadj=TRUE), h=h)
Combination <- (ETS[["mean"]] + ARIMA[["mean"]] +
  STL[["mean"]] + NNAR[["mean"]] + TBATS[["mean"]])/5
```

```{r}
autoplot(auscafe) +
  autolayer(ETS, series="ETS", PI=FALSE) +
  autolayer(ARIMA, series="ARIMA", PI=FALSE) +
  autolayer(STL, series="STL", PI=FALSE) +
  autolayer(NNAR, series="NNAR", PI=FALSE) +
  autolayer(TBATS, series="TBATS", PI=FALSE) +
  autolayer(Combination, series="Combination") +
  xlab("Year") + ylab("$ billion") +
  ggtitle("Australian monthly expenditure on eating out")
```

```{r}
c(ETS = accuracy(ETS, auscafe)["Test set","RMSE"],
  ARIMA = accuracy(ARIMA, auscafe)["Test set","RMSE"],
  `STL-ETS` = accuracy(STL, auscafe)["Test set","RMSE"],
  NNAR = accuracy(NNAR, auscafe)["Test set","RMSE"],
  TBATS = accuracy(TBATS, auscafe)["Test set","RMSE"],
  Combination =
    accuracy(Combination, auscafe)["Test set","RMSE"])
```

# Final Output

## Multiple Predictions
```{r}
h <- 10
fit.lin <- tslm(marathon ~ trend)
fcasts.lin <- forecast(fit.lin, h = h)
fit.exp <- tslm(marathon ~ trend, lambda = 0)
fcasts.exp <- forecast(fit.exp, h = h)

t <- time(marathon)
t.break1 <- 1940
t.break2 <- 1980
tb1 <- ts(pmax(0, t - t.break1), start = 1897)
tb2 <- ts(pmax(0, t - t.break2), start = 1897)

fit.pw <- tslm(marathon ~ t + tb1 + tb2)
t.new <- t[length(t)] + seq(h)
tb1.new <- tb1[length(tb1)] + seq(h)
tb2.new <- tb2[length(tb2)] + seq(h)

newdata <- cbind(t=t.new, tb1=tb1.new, tb2=tb2.new) %>%
  as.data.frame()
fcasts.pw <- forecast(fit.pw, newdata = newdata)

fit.spline <- tslm(marathon ~ t + I(t^2) + I(t^3) +
  I(tb1^3) + I(tb2^3))
fcasts.spl <- forecast(fit.spline, newdata = newdata)

autoplot(marathon) +
  autolayer(fitted(fit.lin), series = "Linear") +
  autolayer(fitted(fit.exp), series = "Exponential") +
  autolayer(fitted(fit.pw), series = "Piecewise") +
  autolayer(fitted(fit.spline), series = "Cubic Spline") +
  autolayer(fcasts.pw, series="Piecewise") +
  autolayer(fcasts.lin, series="Linear", PI=FALSE) +
  autolayer(fcasts.exp, series="Exponential", PI=FALSE) +
  autolayer(fcasts.spl, series="Cubic Spline", PI=FALSE) +
  xlab("Year") + ylab("Winning times in minutes") +
  ggtitle("Boston Marathon") +
  guides(colour = guide_legend(title = " "))

```

## GARCH
Are you curious about the rhythm of the financial market's heartbeat? Do you want to know when a stable market becomes turbulent? In this course on GARCH models you will learn the forward looking approach to balancing risk and reward in financial decision making. The course gradually moves from the standard normal GARCH(1,1) model to more advanced volatility models with a leverage effect, GARCH-in-mean specification and the use of the skewed student t distribution for modelling asset returns. Applications on stock and exchange rate returns include portfolio optimization, rolling sample forecast evaluation, value-at-risk forecasting and studying dynamic covariances.

We start off by making our hands dirty. A rolling window analysis of daily stock returns shows that its standard deviation changes massively through time. Looking back at the past, we thus have clear evidence of time-varying volatility. Looking forward, we need to estimate the volatility of future returns. This is essentially what a GARCH model does! In this chapter, you will learn the basics of using the rugarch package for specifying and estimating the workhorse GARCH(1,1) model in R. We end by showing its usefulness in tactical asset allocation.

Use PerformanceAnalytics (xts objects)

Observe periods of low and high volatility  - volatility is important because it is chance of losing money; turbulent vs calm day 
```{r}
# Plot daily S&P 500 prices
plot(sp500prices)

# Compute daily returns
sp500ret <- CalculateReturns(sp500prices)

# Check the class of sp500ret
class(sp500ret)

# Plot daily returns
plot(sp500ret)
```

In the financial crisis of 2008-2009 the volatility was substantially higher than average. Let's get our hands dirty with analyzing the volatility of the daily returns for the S&P 500 index. You will see that the standard deviation over the complete sample can be substantially different from the standard deviation on subsamples. Recall that standard deviations on daily returns give daily standard deviations. They are annualized by multiplying them using the square-root-of-time rule.
```{r}
# Compute the daily standard deviation for the complete sample   
sd(sp500ret)

# Compute the annualized volatility for the complete sample
sqrt(252) * sd(sp500ret)

# Compute the annualized standard deviation for the year 2009 
sqrt(252) * sd(sp500ret["2009"])

# Compute the annualized standard deviation for the year 2017 
sqrt(252) * sd(sp500ret["2017"])
```

You can visualize the time-variation in volatility by using the function chart.RollingPerformance() in the package PerformanceAnalytics. An important tuning parameter is the choice of the window length. The shorter the window, the more responsive the rolling volatility estimate is to recent returns. The longer the window, the smoother it will be. The function sd.annualized lets you compute annualized volatility under the assumption that the number of trading days in a year equals the number specified in the scale argument.
```{r}
# Showing two plots on the same figure
par(mfrow=c(2,1)) 

# Compute the rolling 1 month estimate of annualized volatility
chart.RollingPerformance(R = sp500ret["2000::2017"], width = 22,
     FUN = "sd.annualized", scale =252, main = "One month rolling volatility")

# Compute the rolling 3 months estimate of annualized volatility
chart.RollingPerformance(R = sp500ret["2000::2017"], width = 66,
     FUN = 'sd.annualized', scale = 252, main = "Three months rolling volatility")
```

GARCH Parameters
1. $\alpha + \beta < 1$ this ensures that the predicted variance $\sigma^2_{t}$ always returns to the long run variance

 - the variance is therefore 'mean-reverting'
 - the long run variane equals $\frac{\omega}{1-\alpha-\beta}$
2. $\omega, \alpha, \beta$ are $ > 0$ : this ensures that $\sigma^2_{t}$ >0 at all times.

GARCH(1,1) reaction to one-off shocks
The GARCH approach models the variance using the prediction errors $e_{t}$ (also called shocks or unexpected returns). The parameter $\alpha$ determines the reactivity to $e^2_{t}$ , while $\beta$ is the weight on the previous variance prediction.


The variance shoots up and then returns to $\frac{\omega}{1-\alpha-\beta}$. The higher the  $\alpha$  parameter, the more reactive the GARCH variance is to large unexpected returns.
The larger is $\alpha$, the larger is the immediate impact of the shock.A large shock causes a spike in the variance. Afterwards, it returns to its long run value.
Holding fixed $\alpha$, the larger is $\beta$, the longer is the duration of the impact.The higher the  $\beta$, the larger is the impact of the previous variance prediction on the next one.

# VI. Model Selection and Evaluation

After having engineered features, we can proceed to modeling time series. However, we should first determine which modeling selectiion strategy we should perform prior to modeling.

There are three general strategies:

  1. Training set fitness  
  2. Evaluating on a holdout set

## Fitness of Model

We can evaluate fitness of model when data are small and examine the statistical assumptions

First, we need to distinguish the Note that forecast errors are different from residuals in two ways. First, residuals are calculated on the training set while forecast errors are calculated on the test set. Second, residuals are based on one-step forecasts while forecast errors can involve multi-step forecasts.

We want residuals to look like white noise:
1. they should be uncorrelated
2. have zero mean
3. have constant variance
4. are normally distributed 

As long as they have zero mean point forecasts re okay. But when normality and autocorrelation are not satisfied, confidence intervals are not reliable

## ASSumption Checks on Statistical Models

### ACF of Residuals in Linear Regression
Another useful test of autocorrelation in the residuals designed to take account for the regression model is the Breusch-Godfrey test, also referred to as the LM (Lagrange Multiplier) test for serial correlation. It is used to test the joint hypothesis that there is no autocorrelation in the residuals up to a certain specified order. A small p-value indicates there is significant autocorrelation remaining in the residuals.

The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models.
```{r}
checkresiduals(fit.consMR)
```

### Residual Plots for Linear Regression against Predictors
Residual plots against predictors
We would expect the residuals to be randomly scattered without showing any systematic patterns. A simple and quick way to check this is to examine scatterplots of the residuals against each of the predictor variables. If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly. See Section 5.8 for a discussion of nonlinear regression.

It is also necessary to plot the residuals against any predictors that are not in the model. If any of these show a pattern, then the corresponding predictor may need to be added to the model (possibly in a nonlinear form).
```{r}
df <- as.data.frame(uschange)
df[,"Residuals"]  <- as.numeric(residuals(fit.consMR))
p1 <- ggplot(df, aes(x=Income, y=Residuals)) +
  geom_point()
p2 <- ggplot(df, aes(x=Production, y=Residuals)) +
  geom_point()
p3 <- ggplot(df, aes(x=Savings, y=Residuals)) +
  geom_point()
p4 <- ggplot(df, aes(x=Unemployment, y=Residuals)) +
  geom_point()
gridExtra::grid.arrange(p1, p2, p3, p4, nrow=2)
```

### Residual Plots for Fitted Values

A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be "heteroscedasticity" in the errors which means that the variance of the residuals may not be constant. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required 
```{r}
cbind(Fitted = fitted(fit.consMR),
      Residuals=residuals(fit.consMR)) %>%
  as.data.frame() %>%
  ggplot(aes(x=Fitted, y=Residuals)) + geom_point()
```

```{r}
res <- residuals(naive(goog200))
autoplot(res) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")
```

```{r}
gghistogram(res) + ggtitle("Histogram of residuals")
ggAcf(res) + ggtitle("ACF of residuals")
```

```{r}
checkresiduals(naive(goog200))
```

These graphs show that the naïve method produces forecasts that appear to account for all available information. The mean of the residuals is close to zero and there is no significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, apart from the one outlier, and therefore the residual variance can be treated as constant. This can also be seen on the histogram of the residuals. The histogram suggests that the residuals may not be normal - the right tail seems a little too long, even when we ignore the outlier. Consequently, forecasts from this method will probably be quite good, but prediction intervals that are computed assuming a normal distribution may be inaccurate.

### Autocorrelation Test
```{r}
# lag=h and fitdf=K
Box.test(res, lag=10, fitdf=0)

Box.test(res,lag=10, fitdf=0, type="Lj")
```


## Evaluating on a Holdout set

###  Creating a Train-Test Set

```{r}
window(ausbeer, start=1995)
subset(ausbeer, start=length(ausbeer)-4*5)

subset(ausbeer, quarter = 1)
tail(ausbeer, 4*5)
```

### Evaluation Metrics on Holdout Set

1. Absolute errors
  - Mean Absolute Errors
  - MAPE
2. Squared errors
  - Mean Squared Errors
  - R-squared
  
The two most commonly used scale-dependent measures are based on the absolute errors or squared errors
When comparing forecast methods applied to a single time series, or to several time series with the same units, the MAE is popular as it is easy to both understand and compute. A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. Consequently, the RMSE is also widely used, despite being more difficult to interpret.

Percentage errors have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. 

Another problem with percentage errors that is often overlooked is that they assume the unit of measurement has a meaningful zero.2 For example, a percentage error makes no sense when measuring the accuracy of temperature forecasts on either the Fahrenheit or Celsius scales, because temperature has an arbitrary zero point

They also have the disadvantage that they put a heavier penalty on negative errors than on positive errors. This observation led to the use of the so-called "symmetric" MAPE (sMAPE)
However, if  yt is close to zero,  ^yt is also likely to be close to zero. Thus, the measure still involves division by a number close to zero, making the calculation unstable. Also, the value of sMAPE can be negative, so it is not really a measure of "absolute percentage errors" at all.Hyndman & Koehler (2006) recommend that the sMAPE not be used. It is included here only because it is widely used, although we will not use it in this book.




In data science, a training set is a data set that is used to discover possible relationships. A test set is a data set that is used to verify the strength of these potential relationships. When you separate a data set into these parts, you generally allocate more of the data for training, and less for testing.

One function that can be used to create training and test sets is subset.ts(), which returns a subset of a time series where the optional start and end arguments are specified using index values.
```{r}
train <- window(oil, end=2003)
test <- window(oil, start=2004)
```

## Evaluating Point Forecasts

Forecast errors are errors on the test set while residuals are on train set. small residuals only means that the model fits the training data well, not that it produces good forecasts.

Forecast accuracy measures: 

1. Mean Absolute ERror
2. Mean Squared Error
3. Mean Absoslute Percentage Error: good for comparisons but only if data are all positive and have no zero or small values
4. Mean Absolute Scaled Error: scaled error version of MAE

## Time Series Cross-Validation

Time series cross-validation: series of training and test sets. forecasting on rolling points.

The tsCV() function computes time series cross-validation errors. It requires you to specify the time series, the forecast method, and the forecast horizon
```{r}
# Compute cross-validated errors for up to 8 steps ahead
e <- tsCV(goog, forecastfunction = naive, h = 8)

# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
```

## Feature Selection
A common approach that is not recommended is to plot the forecast variable against a particular predictor and if there is no noticeable relationship, drop that predictor from the model. This is invalid because it is not always possible to see the relationship from a scatterplot, especially when the effects of other predictors have not been accounted for.

Another common approach which is also invalid is to do a multiple linear regression on all the predictors and disregard all variables whose  
p
 -values are greater than 0.05. To start with, statistical significance does not always indicate predictive value. Even if forecasting is not the goal, this is not a good strategy because the  
p
 -values can be misleading when two or more predictors are correlated with each other
 
```{r}
# measure of predictive  
CV(fit.consMR)
``` 



# VII. Forecasting and Inference

## Confidence Interval
```{r}
 fit.cons <- tslm(Consumption ~ Income, data = uschange)
h <- 4
fcast.ave <- forecast(fit.cons,
  newdata = data.frame(
    Income = rep(mean(uschange[,"Income"]), h)))
fcast.up <- forecast(fit.cons,
  newdata = data.frame(Income = rep(5, h)))
autoplot(uschange[, "Consumption"]) +
  ylab("% change in US consumption") +
  autolayer(fcast.ave, series = "Average increase",
    PI = TRUE) +
  autolayer(fcast.up, series = "Extreme increase",
    PI = TRUE) +
  guides(colour = guide_legend(title = "Scenario"))
```

### Confidence Interval using Bootstrapping
```{r}
bootseries <- bld.mbb.bootstrap(debitcards, 10) %>%
  as.data.frame() %>% ts(start=2000, frequency=12)
autoplot(debitcards) +
  autolayer(bootseries, colour=TRUE) +
  autolayer(debitcards, colour=FALSE) +
  ylab("Bootstrapped series") + guides(colour="none")
```

we simulate many time series that are similar to the original data, using the block-bootstrap described above.
```{r}
nsim <- 1000L
sim <- bld.mbb.bootstrap(debitcards, nsim)
```

For each of these series, we fit an ETS model and simulate one sample path from that model.
```{r}
h <- 36L
future <- matrix(0, nrow=nsim, ncol=h)
for(i in seq(nsim))
  future[i,] <- simulate(ets(sim[[i]]), nsim=h)
```

Finally, we take the means and quantiles of these simulated sample paths to form point forecasts and prediction intervals.
```{r}
start <- tsp(debitcards)[2]+1/12
simfc <- structure(list(
    mean = ts(colMeans(future), start=start, frequency=12),
    lower = ts(apply(future, 2, quantile, prob=0.025),
               start=start, frequency=12),
    upper = ts(apply(future, 2, quantile, prob=0.975),
               start=start, frequency=12),
    level=95),
  class="forecast")
```

These prediction intervals will be larger than those obtained from an ETS model applied directly to the original data.
```{r}
etsfc <- forecast(ets(debitcards), h=h, level=95)
autoplot(debitcards) +
  ggtitle("Monthly retail debit card usage in Iceland") +
  xlab("Year") + ylab("million ISK") +
  autolayer(simfc, series="Simulated") +
  autolayer(etsfc, series="ETS")
```


## Interval Forecast
A common problem is to forecast the aggregate of several time periods of data, using a model fitted to the disaggregated data. For example, we may have monthly data but wish to forecast the total for the next year. Or we may have weekly data, and want to forecast the total for the next four weeks.

If the point forecasts are means, then adding them up will give a good estimate of the total. But prediction intervals are more tricky due to the correlations between forecast errors.

A general solution is to use simulations. Here is an example using ETS models applied to Australian monthly gas production data, assuming we wish to forecast the aggregate gas demand in the next six months.
```{r}
# First fit a model to the data
fit <- ets(gas/1000)
# Forecast six months ahead
fc <- forecast(fit, h=6)
# Simulate 10000 future sample paths
nsim <- 10000
h <- 6
sim <- numeric(nsim)
for(i in seq_len(nsim))
  sim[i] <- sum(simulate(fit, future=TRUE, nsim=h))
meanagg <- mean(sim)
```
The mean of the simulations is close to the sum of the individual forecasts:
```{r}
sum(fc[["mean"]][1:6])
#> [1] 281.8
meanagg
#> [1] 281.7
```

Prediction intervals are also easy to obtain:

```{r}
#80% interval:
quantile(sim, prob=c(0.1, 0.9))
#> 10% 90% 
#> 263 301
#95% interval:
quantile(sim, prob=c(0.025, 0.975))
#>  2.5% 97.5% 
#> 254.1 311.4
```

## Small Integer Forecast

```{r}
productC %>% croston() %>% autoplot()
```

## Positive Forecasts

```{r}
eggs %>%
  ets(model="AAN", damped=FALSE, lambda=0) %>%
  forecast(h=50, biasadj=TRUE) %>%
  autoplot()
```

## Constrained Interval Forecasts

```{r}
# Bounds
a <- 50
b <- 400
# Transform data and fit model
fit <- log((eggs-a)/(b-eggs)) %>%
  ets(model="AAN", damped=FALSE)
fc <- forecast(fit, h=50)
# Back-transform forecasts
fc[["mean"]] <- (b-a)*exp(fc[["mean"]]) /
  (1+exp(fc[["mean"]])) + a
fc[["lower"]] <- (b-a)*exp(fc[["lower"]]) /
 (1+exp(fc[["lower"]])) + a
fc[["upper"]] <- (b-a)*exp(fc[["upper"]]) /
 (1+exp(fc[["upper"]])) + a
fc[["x"]] <- eggs
# Plot result on original scale
autoplot(fc)
```
